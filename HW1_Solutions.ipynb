{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IbwLOEjT4I4"
   },
   "source": [
    "<h4 align=\"right\">by Peter Tang and Ximeng Sun. Modified from the version by Ruizhao Zhu<br> </h4>\n",
    "\n",
    "# Problem Set 1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpjvDX8GT4I_"
   },
   "source": [
    "## Preamble\n",
    "\n",
    "To run and solve this assignment, one must have a working IPython Notebook installation. The  easiest way to set it up is to open the assignment by [Google Colab](https://colab.research.google.com/). It is an online platform which already has the environment set up for this assignment. If you want to run your assignment locally, the easiest way to set it up for both Windows and Linux is to install [Anaconda](https://docs.anaconda.com/anaconda/install/). Then you can create an [environment] (https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) and install [Pytorch](https://pytorch.org/). Pytorch is a deep learning framework we will use across the course. We will only use Pytorch this time for loading some data. Then save this file ([`pset1.ipynb`](https://gist.githubusercontent.com/MInner/eb6330a655a5c37b82e15d1c84fd4cd0/raw/)) to your computer, run Anaconda and choose this file in Anaconda's file explorer. Use `Python 3` version. Below statements assume that you have already followed these instructions. If you are new to Python or its scientific library, Numpy, there are some nice tutorials [here](https://www.learnpython.org/) and [here](http://www.scipy-lectures.org/).\n",
    "\n",
    "To run code in a cell or to render [Markdown](https://en.wikipedia.org/wiki/Markdown)+[LaTeX](https://en.wikipedia.org/wiki/LaTeX) press `Ctr+Enter` or `[>|]`(like \"play\") button above. To edit any code or text cell [double]click on its content. To change cell type, choose \"Markdown\" or \"Code\" in the drop-down menu above.\n",
    "\n",
    "We highly encourage students to put down their derivations into corresponding cells below. However, if one does not know LaTeX (and would find it too hard to learn it by looking at examples listed below between \\$..\\$), he or she might pass it in pen-and-paper format, however grading of these submissions might be delayed.\n",
    "\n",
    "Put your solution into boxes marked with **`[double click here to add a solution]`** and press Ctr+Enter to render text. [Double]click on a cell to edit or to see its source code. You can add cells via **`+`** sign at the top left corner.\n",
    "\n",
    "Submission instructions: please upload your completed solution file to Gradescope by the due date (see Schedule). If you have pen-and-paper answers, please hand them in in class on the same day. \n",
    "\n",
    "`Vector` stands for `column vector` below. Show all steps when proving statements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZ5Ph9quT4JF"
   },
   "source": [
    "### Problem 1: Review of Probability and Statistics\n",
    "\n",
    "**Q1.1**: $X$ and $Y$ are random variables. The expectaion of $X$ is  $E(X)$ and the variance of $X$ is $Var(X)$.  The expectaion of $Y$ is  $E(Y)$ and the variance of $X$ is $Var(Y)$. Prove the following:\n",
    "\n",
    "(a) $E(aX+bY) = aE(X)+bE(Y)$\n",
    "\n",
    "(b) $Var(aX+bY) = a^2 Var(X) + b^2Var(Y) + 2abCov(X,Y)$, where $Cov(X,Y)$ is covariance of X and Y.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chIChbwfIv9f"
   },
   "source": [
    "**Solution Q1.1**\n",
    "(a)\n",
    "\n",
    "$\\begin{aligned} E(aX+bY) &=\\sum_{x} \\sum_{y}(ax+by) P_{X Y}(x, y) \\\\ &=\\sum_{x} \\sum_{y} ax P_{X Y}(x, y)+\\sum_{y} \\sum_{x} by P_{X Y}(x, y) \\\\ &=a\\sum_{x} x P_{X}(x)+ b\\sum_{y}y P_{Y}(y) \\\\ &=aE(X)+bE(Y) \\end{aligned}$\n",
    "\n",
    "Or we can write it as integrals for continuous random variables.\n",
    "\n",
    "$\\begin{aligned}\n",
    "E(X+Y) &=\\int_{x} \\int_{y}(ax+by) f_{X Y}(x, y) \\mathrm{d} x \\\\\n",
    "&=\\int_{x} \\int_{y} ax f_{X Y}(x, y) \\mathrm{d} y \\mathrm{d} x+\\int_{y} \\int_{x} by f_{X Y}(x, y) \\mathrm{d} x \\mathrm{d} y \\\\\n",
    "&=a\\int_{x} x f_{X}(x) \\mathrm{d} x+b\\int_{y} y f_{Y}(y) \\mathrm{d} y \\\\\n",
    "&=aE(X)+bE(Y)\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXAGfxeiIv9h"
   },
   "source": [
    "**Solution Q1.1 (b)**\n",
    "\n",
    "$\\operatorname{Var}(X+Y)$\n",
    "$=\\sum_{x} \\sum_{y}(ax+by)^{2} P_{X Y}(x, y)-(E(aX+bY))^{2}$\n",
    "$=\\sum_{x} \\sum_{y} a^{2}x^{2} P_{X Y}(x, y)+\\sum_{x} \\sum_{y} 2 a b x y P_{X Y}(x, y)+\\sum_{y} \\sum_{x} b^{2} y^{2} P_{X Y}(x, y) - a^{2}(E(X))^{2} - 2 a b E(X) E(Y) - b^{2} (E(Y))^{2}$\n",
    "$=a^{2}\\sum_{x} x^{2} P_{X}(x) - a^{2} (E(X))^{2} + b^{2} \\sum_{y} y^{2} P_{Y}(y) - b^{2}(E(Y))^{2} + 2 a b \\sum_{x} \\sum_{y} x y P_{X Y}(x, y) - 2 a b E(X) E(Y)$\n",
    "$=a^{2} (E\\left(X^{2}\\right) - (E(X))^{2}) + b^{2} (E\\left(Y^{2}\\right)-(E(Y))^{2}) + 2ab(E(X Y)-E(X) E(Y))$\n",
    "$=a^{2}\\operatorname{Var}(X)+b^{2}\\operatorname{Var}(Y)+2 a b\\operatorname{Cov}(X, Y)$\n",
    "\n",
    "Or we can write it as integrals for continuous random variables.\n",
    "\n",
    "$\\begin{array}{l}\n",
    "\\operatorname{Var}(aX+bY) \\\\\n",
    "=\\int_{x} \\int_{y}(ax+by)^{2} f_{X Y}(x, y)-(E(aX+bY))^{2} \\\\\n",
    "=\\int_{x} \\int_{y} a^{2} x^{2} f_{X Y}(x, y)+\\int_{x} \\int_{y} 2 a b x y f_{X Y}(x, y)+\\int_{y} \\int_{x} a^{2} y^{2} f_{X Y}(x, y) - a^{2}(E(X))^{2} - 2 a b E(X) E(Y) - b^{2}(E(Y))^{2} \\\\\n",
    "=a^{2} \\int_{x} x^{2} f_{X}(x) - a^{2}(E(X))^{2} + b^{2} \\int_{y} y^{2} f_{Y}(y) - b^{2} (E(Y))^{2} + 2 a b\\int_{x} \\int_{y} x y f_{X Y}(x, y)-2 a b E(X) E(Y) \\\\\n",
    "=a^{2} E\\left(X^{2}\\right)- a^{2}(E(X))^{2} + b^{2}E\\left(Y^{2}\\right) - b^{2}(E(Y))^{2}+2ab(E(X Y)-E(X) E(Y)) \\\\\n",
    "=a^{2}\\operatorname{Var}(X)+b^{2}\\operatorname{Var}(Y)+2 a b \\operatorname{Cov}(X, Y)\n",
    "\\end{array}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERu4tkxOIv9i"
   },
   "source": [
    "**Q1.2** Suppose $X_1,\\dots,X_N$ have mean $\\mu$ and variance $\\sigma^2$ and are independent. Let $A= (X_1 + .. + X_N)/N$ be the empirical mean.  Show that with probability at least 0.2, $|A-\\mu| < \\frac{\\sqrt{5}\\sigma}{2\\sqrt{N}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_ZKL_5XIv9k"
   },
   "source": [
    "**Solution Q1.2**\n",
    "\n",
    "Since independent, we have $E(A) = \\mu$ and $Var(A) =  \\sigma^2/N$. Applying Chebyshev's inequality \n",
    "$\\operatorname{Pr}(|X-\\mu| \\geq \\frac{\\sqrt{5}\\sigma}{2\\sqrt{N}} ) \\leq \\frac{1}{(\\frac{\\sqrt{5}}{2})^{2}}$.\n",
    "Then we have $Pr(|A-\\mu| < \\frac{\\sqrt{5}\\sigma}{2\\sqrt{N}}) \\geq 0.2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdDnhTezIv9p"
   },
   "source": [
    "**Q1.3**:Assume that we are given $n$ iid samples $(x_1, ..., x_n)$ from each $P(x \\ | \\ \\theta)$ given below. Compute the maximum likelihood estimates (MLEs) for the parameter $\\theta$ $(\\alpha,  \\beta)$ of the given distributions. \n",
    "\n",
    "\n",
    "(a) $P(x \\ | \\ \\theta) = \\theta e^{-\\theta x^2}$ for $x \\geq 0$\n",
    "\n",
    "(b) $P(x \\ | \\ \\theta) = \\frac{1}{1-\\theta}$ for $ \\theta \\leq x \\leq 1$ \n",
    "\n",
    "(c): $P(x \\ | \\ \\alpha, \\beta) = \\frac{1}{\\pi \\alpha [1 + (\\frac{x-\\beta}{\\alpha})^{2}]}$\n",
    "\n",
    "(If $x$ is not in the support of the distribution defined by inequalities, the probability of it is $0$.)\n",
    "\n",
    "**Hint for Q1.3(c):** Don't solve for \\alpha or \\beta, just simply the equation as much as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1BoB_qyT4JJ"
   },
   "source": [
    "\n",
    "**Solution Q1.3(a):**\n",
    "\n",
    "We could either find the maximum of the Likelihood function or it's logarithm. This is true since logarithm is a monotonically increasing function.\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{ML} &= \\arg\\max_\\theta(\\mathcal{L}(\\theta))= \\arg\\max_\\theta(\\log P(x_1, \\dots, x_n | \\theta))\\\\\n",
    "&= \\arg \\max_{\\theta}\\sum_{i=1}^n \\log\\left(\\theta e^{-\\theta x_i^2} \\right)\\\\\n",
    "&= \\arg \\max_{\\theta}\\sum_{i=1}^n \\left(\\log \\theta - \\log e ^{\\theta x_i^2}\\right)\\\\\n",
    "&= \\arg \\max_{\\theta}\\left(n\\log\\theta - \\theta \\sum_{i=1}^{n}x_i^2 \\right)\n",
    "\\end{align*}\n",
    "$ \\Rightarrow $ we can solve for the maximum by finding the point at which gradient of our likelihood function is zero: \n",
    "\n",
    "\\begin{align*}\n",
    "  \\frac{\\partial }{\\partial \\theta} \\mathcal{L}(\\theta) &= 0\\\\\n",
    "  \\frac{n}{\\theta_{ML}} - \\sum_{i=1}^n x_i^2 &= 0\n",
    "\\end{align*}\n",
    "\n",
    "$$\\Rightarrow \\theta_{ML} = \\frac{n}{\\sum x_i^2} $$\n",
    "\n",
    "**Solution Q1.3(b):**\n",
    "\n",
    "First, note, that if $\\theta$ is larger than some $x_j$ then $P(x_j\\mid \\theta)=0$ and therefore $\\mathcal{L}(\\theta)=0$.\n",
    "\n",
    "Now, let's look at the likelihood.\n",
    "$$ \\mathcal{L}(\\theta) = P(x_1, \\dots, x_n\\mid\\theta) = \\prod_{i=1}^n P(x_i\\mid\\theta) = \\left(\\frac{1}{1-\\theta}\\right)^n $$\n",
    "$$\\frac{\\partial}{\\partial \\theta}\\mathcal{L}(\\theta) = \\frac{n}{(1-\\theta)^{n+1}} > 0 $$\n",
    "\n",
    "From the derivative, we can see that likelihood is monotonically increasing with respect to $\\theta$. Combined with our observation that $\\theta>\\min\\{x_i\\}$ results in $0$ likelihood,\n",
    "$$\n",
    "  \\theta_{ML} = \\min_{i=1}^n\\{x_i\\}\n",
    "$$\n",
    "\n",
    "**Solution Q1.3(c):**\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta_{ML} &= \\arg\\max_\\theta(\\mathcal{L}(\\theta))= \\arg\\max_\\theta(\\log P(x_1, \\dots, x_n | \\alpha, \\beta))\\\\\n",
    "&= \\arg \\max_{\\theta}\\sum_{i=1}^n \\log\\left(\\frac{1}{\\pi \\alpha [1 + (\\frac{x-\\beta}{\\alpha})^{2}]} \\right)\\\\\n",
    "&= \\arg \\max_{\\theta}\\sum_{i=1}^n \\left(-\\log \\pi - \\log\\alpha + \\log \\frac{\\alpha^2 + (x_i - \\beta)^2}{\\alpha^2}\\right)\\\\\n",
    "&= \\arg \\max_{\\theta}\\left(- n\\log\\pi + n\\log\\alpha - \\sum_{i=1}^{n}log \\left(\\alpha^2 +(x_i-\\beta)^2 \\right) \\right)\n",
    "\\end{align*}\n",
    "\n",
    "Then, take partial derivative wrt. two variables\n",
    "\\begin{align*}\n",
    "\\frac{\\partial\\mathcal{L}(\\theta)}{\\partial\\beta} &= \\sum_{i=1}^n \\frac{2(x_i-\\beta)}{\\alpha^2 + (x_i - \\beta)^2} &= 0\\\\\n",
    "\\frac{\\partial\\mathcal{L}(\\theta)}{\\partial\\alpha} &= \\frac{n}{\\alpha} - \\sum_{i=1}^n \\frac{2 \\alpha}{\\alpha^2 + (x_i - \\beta)^2} &= 0\\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bI8M-uRuIv9t"
   },
   "source": [
    "**Q1.4** (Naive Bayes Maximum Likelihood) Consider binary dataset $S$ with observations in the form $\\left.\\left\\{\\left(x_{j}^{1}, \\ldots, x_{j}^{n}\\right), y_{j}\\right)\\right\\}$. Each data $\\mathbf{x_j}$ is an n-dimensional vector. Define $c(y)$ as a function that counts the number of observations such that the label is $y$. $S$ is the set of all the training data.\n",
    "$$\n",
    "c(y)=\\sum_{\\left(x_{j}, y_{j}\\right) \\in S}\\left[y_{j}=y\\right]\n",
    "$$\n",
    "Define $c(i, y)$ as a function that counts the number of observations such that the label is $y$ and $x^{i}=1$\n",
    "$$\n",
    "c(i, y)=\\sum_{\\left(x_{j}, y_{j}\\right) \\in S}\\left[y_{j}=y, x_{j}^{i}=1\\right]\n",
    "$$\n",
    "Define $b$ as $P(Y=1),$ and $b^{i y}$ as $P\\left(X^{i}=1 \\mid Y=y\\right) .$ Prove that the following estimators are MLE for these parameters:\n",
    "$$\n",
    "\\widehat{b}_{M L E}=\\frac{c(1)}{|S|} \\quad \\text { and } \\quad \\widehat{b^{i y}}_{M L E}=\\frac{c(i, y)}{c(y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-L86HfvtIv9u"
   },
   "source": [
    "**Solution 1.4**\n",
    "Given the training set $S$, we can write the log-likelihood function as follows:\n",
    "\\begin{align*}\n",
    "L &= \\sum_{|S|} \\log P(x_j,y_j)\\\\\n",
    "&= \\sum_{|S|} \\log \\big( P(y_j) \\prod_{i = 1}^n P(x^i_j|y_j)  \\big) \\\\\n",
    "&=  \\sum_{|S|} \\log P(y_j) + \\sum_{|S|}\\log \\big( \\prod_{i = 1}^n P(x^i_j|y_j) \\big)\\\\\n",
    "&= \\sum_{|S|} \\log P(y_j) + \\sum_{|S|}\\sum_{i = 1}^n \\log P(x^i_j|y_j)\\\\\n",
    "&= c(1)\\log\\big(P(Y=1)\\big) + c(0)\\log\\big(P(Y=0)\\big) + \\sum_{y = 0,1}\\bigg( c(i,y) \\log P(X^i = 1|Y = y) + (c(y)-c(i,y)) \\log P(X^i = 0|Y = y)\\bigg)\\\\\n",
    "&= c(1)\\log\\big(P(Y=1)\\big) + (|S| - c(1))\\log\\big(1-P(Y=1)\\big) + \\sum_{y = 0,1}\\bigg( c(i,y)\\log b^{i y} + (c(y)-c(i,y)) \\log (1-b^{i y})\\bigg) \\\\\n",
    "&= c(1)\\log\\big(b\\big) + (|S| - c(1))\\log\\big(1-b\\big) + \\sum_{y = 0,1}\\bigg( c(i,y)\\log b^{i y} + (c(y)-c(i,y))\\log (1-b^{i y})\\bigg)\n",
    "\\end{align*}\n",
    "\n",
    "Concave funtion wrt b,\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{c(1)}{b} - \\frac{|S| - c(1)}{1-b} = 0\\\\\n",
    "\\Rightarrow \\widehat{b}_{M L E}=\\frac{c(1)}{|S|} \n",
    "\\end{align*}\n",
    "Concave cunction wrt $b^{iy}$, for either $y=0$ or $y=1$, we have the as follows:\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial b^{iy}} =  \\frac{c(i,y)}{b^{iy}} - \\frac{c(y) - c(i,y)}{1-b^{iy}} = 0\\\\\n",
    "\\Rightarrow \\widehat{b^{i y}}_{M L E}=\\frac{c(i, y)}{c(y)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Evp1_DmHT4JP"
   },
   "source": [
    "### Problem 2: Matrix Derivatives\n",
    "\n",
    "**Multivariate Gaussian**\n",
    "\n",
    "Assume that our data is distributed according to a $\\underline d$ dimensional [multivariate Gaussian](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Likelihood_function) with $\\bar \\mu$ mean and $\\Sigma$ covariance matrix: $$(\\mathbf x_1, \\dots, \\mathbf x_n) \\sim \\mathcal N(\\bar \\mu, \\Sigma).$$ \n",
    "\n",
    "**Q2.1:** \n",
    "Using rules of [matrix derivatives](http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf) Equation [57, 59], derive $\\frac{\\partial \\mathcal L(\\theta)}{\\partial \\Sigma}$ in matrix form and set it to zero to find $\\Sigma_{ML}$ . Assume each $x_i$ is drawn independently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1hhI03PT4JU"
   },
   "source": [
    "**Solution Q2.1:**\n",
    "$$ P(\\mathbf x_1, \\dots, \\mathbf x_n |\\ \\mathbf \\mu, \\Sigma) = \\prod_{i=1}^n P(\\mathbf x_i|\\ \\mathbf \\mu, \\Sigma)$$\n",
    "\n",
    "$$ \\frac{1}{(2\\pi)^{\\frac{DN}{2}}|\\Sigma|^{\\frac{N}{2}}}\\exp(\\frac{-1}{2}\\sum_{i=1}^n(\\mathbf x_i-\\mu)^T\\Sigma^{-1}(\\mathbf x_i-\\mu))$$\n",
    "\n",
    "$$\\Rightarrow \\mathcal L(\\theta) = -\\frac{N}{2} \\log|\\Sigma|-\\frac{1}{2}\\sum_{i=1}^n(\\mathbf x_i-\\mu)^T\\Sigma^{-1}(\\mathbf x_i-\\mu) + C$$\n",
    "\n",
    "$$\\Rightarrow \\frac{\\partial \\mathcal L(\\theta)}{\\partial \\Sigma} =-\\frac{N}{2} \\Sigma^{-T} + \\frac{1}{2}\\sum_{i=1}^n\\Sigma^{-T}(\\mathbf x_i-\\mu)(\\mathbf x_i-\\mu)^T\\Sigma^{-T} $$\n",
    "\n",
    "Equating above with $0$,\n",
    "$$  \\Sigma_{ML} = \\frac{1}{n}\\sum_{i=1}^n(\\mathbf x_i-\\mu)(\\mathbf x_i-\\mu)^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FWewuNzT4Jc"
   },
   "source": [
    "**Q2.2: Multi-target Linear Regression**\n",
    "- we have $X \\in \\mathbf R^{n \\times d}$ is a constant data matrix\n",
    "- $\\theta$ is a $d \\times m$-dimensional **weight matrix**\n",
    "- $\\varepsilon_{ij} \\sim \\mathcal N(0, \\sigma^2_\\epsilon)$ is a normal noise ($i \\in [0; n], j \\in [0;m]$)\n",
    "- and we observe a matrix $Y = X\\theta + \\varepsilon \\in \\mathbf R^{n \\times m}$\n",
    "\n",
    "$$\\varepsilon = Y - X\\theta \\sim \\mathcal N(0, \\sigma^2_\\epsilon I)$$\n",
    "\n",
    "$$\\mathcal L(\\theta) = \\log P(Y \\ | \\ X,\\theta) = \\log \\mathcal N(Y - X\\theta \\ | \\ 0, \\sigma^2_\\epsilon I)$$\n",
    "\n",
    "$$\\theta_{MLE} = \\arg \\max_{\\theta} \\mathcal L(\\theta) = \\arg \\min_{\\theta} \\text{loss}(\\theta) = \\arg \\min_{\\theta} \\big( ||Y-X\\theta||^2_F \\big)$$\n",
    "\n",
    "Assume $\\theta \\sim \\mathcal N(0, \\sigma^2_\\theta I)$, which essentially means that \"weight components should not be too far from zero\".\n",
    "Where $I$ stands for an identity matrix. \n",
    "\n",
    "**Using rules of [matrix derivatives](http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf)** Equation [137, 132], show for an MLE loss:  $\\text{loss}(\\theta) = ||Y-X\\theta||^2_F$ that:\n",
    "\n",
    "**Q2.2.1:** derive $\\frac{\\partial\\text{loss}(\\theta)}{\\partial \\theta} = -2 X^T(Y-X\\theta)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 2.2.1:**\n",
    "Using chain rule, we take the derivative of loss function with respect to $\\theta$\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\mathcal L(\\theta)}{\\partial \\theta} &= 2(Y-X\\theta)\\frac{\\partial (Y-X\\theta)}{\\partial \\theta}\\\\\n",
    "    &= -2X^T(Y-X\\theta)\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.2.2:** derive $\\theta_{MLE} = (X^T X)^{-1} X^T Y$\n",
    "\n",
    "**Hint:** In our case [see Matrix Cookbook, eq. 137], $g(U) = ||U||^2_F$ - squared Frobenius norm and $U(\\theta) = f(\\theta) = Y - X \\theta$ - linear mapping.\n",
    "\n",
    "**Note:** That is a multi-target problem, therfore $\\theta$ is a matrix, so you have to take the derivative wrt matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yf6apygOT4Jh"
   },
   "source": [
    "**Solution 2.2.2:**\n",
    "$$\\theta_{MLE} = \\arg\\min_\\theta loss(\\theta)$$\n",
    "\n",
    "$$\\frac{\\partial\\text{loss}(\\theta)}{\\partial \\theta} =0 $$\n",
    "\n",
    "$$ \\Rightarrow -2X^T(Y-X\\theta)= 0 \\Rightarrow X^TY = X^TX\\theta $$\n",
    "\n",
    "$$\\Rightarrow \\theta_{MLE} = (X^T X)^{-1} X^TY$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RyTcnxIv93"
   },
   "source": [
    "**Q2.2.3**\n",
    "To make use of prior information,  one might want to find [maximum a posteriori estimation](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) (MAP). In this problem, the MAP is defined as  $\\theta_{MAP} = \\arg\\max_\\theta L_{MAP}(\\theta) = \\arg\\max_\\theta P(\\theta | X, Y)$. The priori of $\\theta$ is $P(\\theta)$. Show $\\mathcal L_{MAP}(\\theta)$ can be defined as $\\mathcal L_{MLE}(\\theta) + \\log P(\\theta)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbW_YyV-Iv93"
   },
   "source": [
    "**Solution 2.2.3**\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathcal L_{MAP}(\\theta) &=  P(\\theta | X, Y) =  \\frac{P(Y|\\theta, X)P(\\theta)}{\\int_{\\Theta} P(Y|\\theta,X)P(\\theta) d \\theta} = \\big[ \\log P(Y|\\theta,X) + \\log P(\\theta) - \\log \\int_{\\Theta} P(Y|\\theta,X)P(\\theta) d \\theta \\big] \\\\ &=  \\big[ \\log P(Y|\\theta,X) + \\log P(\\theta) \\big] =\\mathcal L_{MLE}(\\theta) + \\log P(\\theta)\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.2.4**\n",
    "Assume the prior $\\theta \\sim \\mathcal N(0, \\sigma^2_\\theta I)$, which essentially means that \"weight vector components should not be too far from zero\". \n",
    "Show MAP with gaussian prior is L2 regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcXUJL-nIv96"
   },
   "source": [
    "**Solution 2.2.4**\n",
    "\n",
    "First, we take the multivariate normal density over a $k$ dimensional vector variable $\\theta$ \n",
    "\n",
    "\\begin{align*}\n",
    "f(\\theta; \\mu,\\Sigma)=\\frac{1}{(2\\pi)^{k/2}|\\Sigma|^{1/2}}\\exp\\left(-\\frac{1}{2}(\\theta-\\mu)^T \\Sigma^{-1} (\\theta-\\mu)\\right)\n",
    "\\end{align*}\n",
    "\n",
    "with $\\mu=0$ and $\\Sigma= \\sigma^2_\\theta I$ and use it as the $P(\\theta)$ term in the $\\mathcal L_{MAP}(\\theta)$ expression above. Taking its log we get:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\log P(\\theta) = \\log\\left(\\frac{1}{(2\\pi)^{k/2}\\sigma_\\theta}\\exp\\left(-\\frac{\\theta^T\\theta}{2\\sigma_\\theta^2}\\right)\\right) = -\\frac{1}{2\\sigma_\\theta^2}\\theta^T\\theta + C = -\\frac{1}{2\\sigma_\\theta^2}||\\theta||^2_2 + C \n",
    "\\end{align*}\n",
    "\n",
    "This means that the new objective adds an L2 regularization term  on the parameters $\\theta$ multiplied by a constant (hyperparameter equivalent to $\\lambda$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.2.5**\n",
    "Now assume that a vectorized $\\theta$ follows a [Laplace distribution](https://en.wikipedia.org/wiki/Laplace_distribution) i.e: $\\theta_i \\sim Laplace(0, b),  \\forall i $\n",
    "Show MAP with laplace prior is L1 regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRfX3jGbIv-A"
   },
   "source": [
    "**Solution 2.2.5**\n",
    "Following a similar process to the previous question:\n",
    "\n",
    "\\begin{align*}\n",
    "  P(\\theta) &= \\prod_{i=1}^n P(\\theta_i) = \\prod_{i=1}^n \\frac{1}{2 b}\\exp\\left(-\\frac{|\\theta_i|}{b}\\right)\\\\\n",
    "  \\log P(\\theta) &= \\sum_{i=1}^n \\left(-\\log2b -\\frac1b|\\theta_i|\\right)\\\\\n",
    "  &= C - \\frac1b||\\theta||_1\\\\\n",
    "\\end{align*}  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rozDTqlT4KL"
   },
   "source": [
    "### Problem 3: Ridge and Lasso Regression\n",
    "\n",
    "In this problem, we want to solve the regression problem with two different machine learning methods: Ridge Regression and Lasso Regression.\n",
    "\n",
    "- We have the constant data matrix $X \\in \\mathbf R^{n \\times d}$. Each row $\\mathbf{x_i}$ is a d-dimensional data vector.\n",
    "- We have the vector $\\mathbf{w} \\in \\mathbf{R}^d$ and noise vector $\\mathbf{\\epsilon} \\in \\mathbf{R}^n$. \n",
    "- We observe $y_i \\in \\mathbf{R}$ as the output for each row in the input data matrix $\\mathbf{x_i}$: $y_i = \\mathbf{w}^T \\mathbf{x_i} + \\epsilon_i$\n",
    "\n",
    "Given the model definition above, please derive the following:\n",
    "\n",
    "**Q3.1**: Ridge Regression can be formulated as: \n",
    "\\begin{align*}\n",
    "\\mathbf{w}^* = \\text{argmin}_{\\mathbf{w} \\in \\mathbf{R}^d} \\sum_{i} (y_i - \\mathbf{w}^T \\mathbf {x_i})^2 + \\lambda_r ||\\mathbf{w} ||_2^2,\n",
    "\\end{align*}\n",
    "where $\\lambda_r >0$ is the hyperparameter to control the L2 regularization of $\\mathbf{w}$. \n",
    "The loss function for Ridge Regression is $J(\\mathbf{w}, \\lambda_r)  = \\sum_{i} (y_i - \\mathbf{w}^T \\mathbf{x_i})^2 + \\lambda_r ||\\mathbf{w} ||_2^2 $\n",
    "Derive the closed form for the optimal $\\mathbf{w}^*$ in terms of $\\mathbf{X}$, $\\mathbf{y}$ and $\\lambda_r$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcavoS8aIv-e"
   },
   "source": [
    "**Solution 3.1:**  \n",
    "We first write the optimization function in the matrix form\n",
    "\\begin{align*}\n",
    "    J(\\mathbf{w}, \\lambda_r) & = \\sum_{i} (y_i - \\mathbf{w}^T \\mathbf{x_i})^2 + \\lambda_r ||\\mathbf{w} ||_2^2 \\\\\n",
    "    & = || \\mathbf{X} \\mathbf{w} - \\mathbf{y} ||_2^2 + \\lambda_r ||\\mathbf{w}||_2^2 \\\\\n",
    "    &= \\mathbf{w}^T \\mathbf{X}^T \\mathbf{X} \\mathbf{w} - 2 \\mathbf{y}^T \\mathbf{X} \\mathbf{w} + \\mathbf{y}^T\\mathbf{y}  + \\lambda_r \\mathbf{w}^T \\mathbf{w}.\n",
    "\\end{align*}\n",
    "Take the derivative of $J(\\mathbf{w}, \\lambda_r)$:\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial J(\\mathbf{w}, \\lambda_r) }{\\partial \\mathbf{w}} =  2 (\\mathbf{X}^T \\mathbf{X} + \\lambda_r \\mathbf{I} ) \\mathbf{w} - 2 \\mathbf{X}^T \\mathbf{y} = 0 \n",
    "\\end{align*}\n",
    "Then we get $\\mathbf{w}^* = (\\mathbf{X}^T \\mathbf{X} + \\lambda_r \\mathbf{I}) ^ {-1} \\mathbf{X}^T\\mathbf{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYBtztIIIv-g"
   },
   "source": [
    "**Q3.2**: We can also use L1 regularization of $\\mathbf{w}$ instead of L2 regularization in the Ridge Regression, which formulates the Lasso Regression:\n",
    "\\begin{align*}\n",
    "\\mathbf{w}^* \\in \\text{argmin}_{\\mathbf{w} \\in \\mathbf{R}^d} \\sum_{i} (y_i - \\mathbf{w}^T \\mathbf{x_i})^2 + \\lambda_l ||\\mathbf{w} ||_1,\n",
    "\\end{align*}\n",
    "where  $\\lambda_l > 0$  is the hyperparameter to control the L1 regularization of  $\\mathbf{w}$. \n",
    "The loss function for Lasso Regression is $J(\\mathbf{w}, \\lambda_l) = \\sum_{i} (y_i - \\mathbf{w}^T \\mathbf{x_i})^2 + \\lambda_l ||\\mathbf{w} ||_1$.\n",
    "Show that Lasso Regression is not differentiable at some points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxMoAhqvIv-g"
   },
   "source": [
    "**Solution 3.2**: The loss function for Lasso Regression is $J(\\mathbf{w}, \\lambda_l) = \\sum_{i} (y_i - \\mathbf{w}^T \\mathbf{x_i})^2 + \\lambda_l ||\\mathbf{w} ||_1$.\n",
    "\n",
    "We denote $\\Delta \\mathbf{w_i} \\in \\mathbf{R}^d $ as a vector where $i$-th entry is non-zero and other entries are all zeros.\n",
    "\n",
    "For $\\hat{\\mathbf{w}}$ where $\\mathbf{w}_i = 0$, we take the partial derivative with respect to $\\mathbf{w}_i$.\n",
    "\\begin{align*}\n",
    "a & = \\lim_{\\Delta \\mathbf{w_i} \\rightarrow 0 ^ +} \\frac{J(\\hat{\\mathbf{w}} + \\mathbf{w_i}, \\lambda_l) - J(\\hat{\\mathbf{w}}, \\lambda_l)}{(\\mathbf{w_i})_i} = \\frac{\\partial \\sum_{i} (y_i - \\hat{\\mathbf{w}}^T \\mathbf{x_i})^2}{\\partial \\mathbf{w}_i} + \\lambda_l, \\\\\n",
    "b & = \\lim_{\\Delta \\mathbf{w_i} \\rightarrow 0 ^ -} \\frac{J(\\hat{\\mathbf{w}} + \\mathbf{w_i}, \\lambda_l) - J(\\hat{\\mathbf{w}}, \\lambda_l)}{(\\mathbf{w_i})_i} = \\frac{\\partial \\sum_{i} (y_i - \\hat{\\mathbf{w}}^T \\mathbf{x_i})^2}{\\partial \\mathbf{w}_i} - \\lambda_l.\n",
    "\\end{align*} \n",
    "Since $\\lambda_i > 0$, $a \\ne b$ and thus $J(\\mathbf{w}, \\lambda_l)$ is non-differentiable at $\\hat{\\mathbf w}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjt-DRK0T4Kc"
   },
   "source": [
    "**Q3.3**: In this question, we will show the maximum value for $\\lambda_l$ is $2||\\mathbf{X}^T\\mathbf{y}||_{\\infty} $in Lasso Regression, i.e. for any $\\lambda_l \\ge 2||\\mathbf{X}^T\\mathbf{y}||_{\\infty}$, the optimal $\\mathbf{w}$ is always a 0-vector. We will gradually show this in three steps.\n",
    "\n",
    "**Q3.3.1**: \n",
    "The one-sided directional derivative of a function $f(x)$ at the direction $\\mathbf{u} \\in \\mathbf{R}^d$ is defined as follows.\n",
    "\\begin{align*}\n",
    "f'(x ; \\mathbf{u}) = \\lim_{h \\rightarrow 0 ^+} \\frac{f(x+h\\mathbf{u}) - f(x)}{h},\n",
    "\\end{align*}\n",
    "where $\\mathbf{u}$ is a unit vector. \n",
    "Please compute the one-sided directional derivative at $J(\\mathbf{0} , \\lambda_l)$ at direction $\\mathbf{u}$. The final result (i.e. $J'(\\mathbf{0}, \\lambda_l; \\mathbf{u})$) should be in terms of $\\mathbf{X}$ (matrix), $\\mathbf{y}$ (vector), $\\lambda_l$ and $\\mathbf{u}$ (vector). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opyLyvwUT4Kf"
   },
   "source": [
    "**Solution 3.3.1:**  \n",
    "\\begin{align*}\n",
    "J'(\\mathbf{0}, \\lambda_l; \\mathbf{u}) &= \\lim_{h \\rightarrow 0 ^ +} \\frac{J(h\\mathbf{u}, \\lambda_l) - J(\\mathbf{0}, \\lambda_l)}{h} \\\\\n",
    "&= \\lim_{h \\rightarrow 0 ^ +} \\frac{||h \\mathbf{Xu} - \\mathbf{y} ||_2^2 + \\lambda_l||h\\mathbf{u}||_1- ||\\mathbf{y} ||_2^2}{h} \\\\\n",
    "& = \\lim_{h \\rightarrow 0 ^ +} \\frac{h^2 (\\mathbf{Xu})^T \\mathbf{Xu} - 2 h (\\mathbf{Xu})^T \\mathbf{y} + ||\\mathbf{y}||_2^2 + \\lambda_l h ||\\mathbf{u}||_1 - ||\\mathbf{y}||_2^2}{h} \\\\\n",
    "& = - 2 \\mathbf{u}^T \\mathbf{X}^T \\mathbf{y} + \\lambda_l ||\\mathbf{u}||_1\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUMuVy1tIv-m"
   },
   "source": [
    "**Q3.3.2**  Show that for any $\\mathbf{u} \\ne \\mathbf{0}$, we have $J'(\\mathbf{0},\\lambda_l; \\mathbf{u}) \\ge 0$ if and only if $\\lambda_l$ is greater than or equal to some constant $C$, which depends on  $\\mathbf{X}$ (matrix), $\\mathbf{y}$ (vector) and $\\mathbf{u}$ (vector). Please give an explicit expression for $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fP_6WEpDZwSP"
   },
   "source": [
    "**Solution 3.3.2**:\n",
    "\\begin{align*}\n",
    "&J'(\\mathbf{0}, \\lambda_l; \\mathbf{u}) \\ge 0, \\forall \\mathbf{u} \\ne 0 \\\\\n",
    "\\Leftrightarrow & - 2 \\mathbf{u}^T \\mathbf{X}^T \\mathbf{y} + \\lambda_l ||\\mathbf{u}||_1 \\ge 0,  \\forall \\mathbf{u} \\ne 0 \\\\\n",
    "\\Leftrightarrow &  \\lambda_l \\ge 2 \\frac{ \\mathbf{u}^T \\mathbf{X}^T \\mathbf{y} }{||\\mathbf{u}||_1}. \\\\\n",
    "\\end{align*} \n",
    "Therefore, $C=2 \\frac{ \\mathbf{u}^T \\mathbf{X}^T \\mathbf{y} }{||\\mathbf{u}||_1} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9r2gvJLcfDH"
   },
   "source": [
    "**Q3.3.3**: Due to the convexity of Lasso Regression, $\\mathbf{w}^*$ is a minimzer of $J(\\mathbf{w}, \\lambda_l)$ if and only if the directional derivative $J'(\\mathbf{w}^*, \\lambda_l; \\mathbf{u}) \\ge 0$ for all $\\mathbf{u} \\ne 0$. Show that $\\mathbf{w} = 0$ is the minimizer of $J(\\mathbf{w}, \\lambda_l)$ if and only if $\\lambda_l \\ge 2 || \\mathbf{X}^T \\mathbf{y}||_\\infty$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfquK_XPeyhw"
   },
   "source": [
    "**Solution 3.3.3**: We first show the maximum value of $2 \\frac{ \\mathbf{u}^T \\mathbf{X}^T \\mathbf{y} }{||\\mathbf{u}||_1} $ is  $2 ||\\mathbf{X}^T \\mathbf{y}||_\\infty$, for all $\\mathbf{u} \\ne 0$.\n",
    "We denote $\\tilde{\\mathbf{u}} = \\frac{\\mathbf{u}}{||\\mathbf{u}||_1} $, and thus $||\\tilde{\\mathbf{u}} ||_1 = 1$.\n",
    "\\begin{align*}\n",
    "2 \\frac{ \\mathbf{u}^T \\mathbf{X}^T \\mathbf{y} }{||\\mathbf{u}||_1} = 2 \\tilde{\\mathbf{u}} (\\mathbf{X}^T \\mathbf{y}) \\le 2 ||\\tilde{\\mathbf{u}} (\\mathbf{X}^T \\mathbf{y})||_1\n",
    "\\end{align*}\n",
    "Due to Holder inequality:\n",
    "\\begin{align*}\n",
    "2 \\frac{ \\mathbf{u}^T \\mathbf{X}^T \\mathbf{y} }{||\\mathbf{u}||_1}  \\le 2 ||\\tilde{\\mathbf{u}} (\\mathbf{X}^T \\mathbf{y})||_1 \\le 2 || \\tilde{\\mathbf{u}} ||_1 || \\mathbf{X}^T \\mathbf{y}||_\\infty = 2 || \\mathbf{X}^T \\mathbf{y}||_\\infty.\n",
    "\\end{align*}\n",
    "\n",
    "The equality can be achieved in all steps above. The maximum value of $2 \\frac{ \\mathbf{u}^T \\mathbf{X}^T \\mathbf{y} }{||\\mathbf{u}||_1} $ is $2 ||\\mathbf{X}^T \\mathbf{y}||_\\infty$.\n",
    "\n",
    "According to the convexity of Lasso Regression, if and only if $\\lambda_l \\ge 2 ||\\mathbf{X}^T \\mathbf{y}||_\\infty$, then $J'(\\mathbf{0}, \\lambda_l; \\mathbf{u})\\ge 0,  \\forall \\mathbf{u} \\ne \\mathbf{0}$, which indicates $\\mathbf{w}^* = \\mathbf{0}$ is a minimizer of $J(\\mathbf{w}, \\lambda_l)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQtsoMDsT4Kx"
   },
   "source": [
    "### Problem 4: Coding Ridge and Lasso Regression [15 points]\n",
    "In this problem, we will solve Ridge and Lasso Regression with Stochastic Gradient Descent (SGD) with the synthetic data. We will compute the (sub)gradient with the training data manually and update the weight vector $\\mathbf{w}$. With the validation data, we tune the hyperparameters $\\lambda_r$ and $\\lambda_l$. Finally we compare the learned weights learned by Ridge Regression and Lasso Regression. \n",
    "\n",
    "**Q4.1** Compute the gradient $\\frac{\\partial J(\\mathbf{w}, \\lambda_r)}{\\partial \\mathbf{w}}$ of Ridge Regression. Also, compute  $\\frac{\\partial J(\\mathbf{w}, \\lambda_l)}{\\partial \\mathbf{w}}$ at all differentiable points and the subgradient at non-differentiable points in Lasso Regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njmbdcqyWyqb"
   },
   "source": [
    "**Solution 4.1**\n",
    "In Ridge Regression,\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w}, \\lambda_r)}{\\partial \\mathbf{w}} = 2 \\mathbf{X}^T (\\mathbf{Xw} - \\mathbf{y})  + 2 \\lambda_r \\mathbf{w}\n",
    "\\end{align*}\n",
    "In Lasso Regression, for all differentable $\\mathbf{w}$ (i.e. $\\mathbf{w}_i \\ne 0, \\forall i$):\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w}, \\lambda_r)}{\\partial \\mathbf{w}} = 2 \\mathbf{X}^T (\\mathbf{Xw} - \\mathbf{y}) + \\lambda_l sign(\\mathbf{w})\n",
    "\\end{align*}\n",
    "For any non-differentiable $\\mathbf{w}$, the subgradient is $2 \\mathbf{X}^T (\\mathbf{Xw} - \\mathbf{y}) + \\lambda_l s(\\mathbf{w})$, where $s(\\mathbf{w}) \\in \\mathbf{R}^d$ and\n",
    "$$\n",
    "s_i (\\mathbf{w}) = \\left\\{ \n",
    "\\begin{align*}\n",
    "sign(\\mathbf{w}_i), \\mathbf{w}_i \\ne 0, \\\\\n",
    "0, \\mathbf{w}_i = 0\n",
    "\\end{align*}\n",
    "  \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lyK2dVRW4tc"
   },
   "source": [
    "**Q4.2** Implement the Ridge and Lasso Regression.\n",
    "We provide the synthetic data in `lasso_data_ours.pickle` in which $\\mathbf{y} = \\mathbf{Xw} + noise $. Please implement the ridge and lasso regression to learn the weight $\\mathbf{w}$ with **Stochastic** Gradient Descent. For the differentiable $\\mathbf{w}$, we use the gradient to update $\\mathbf{w}$ and for non-differentiable $\\mathbf{w}$, we use the subgradient instead. You need to fill in the missing part (starting with `## -- ! code required` comment) in `Ridge_loss`, `Ridge_grad`, `Lasso_loss`, `Lasso_grad` and `training`. Note: in this homework, you are required to use the gradient of the loss function computed manually (in Q5.1) instead of using the `autograd` in any deep learning library. Visualize the learned $\\mathbf{w}$ and find out the difference between $\\mathbf{w}$s learned by Ridge and Lasso Regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "y_2EhzzZT4Kz",
    "outputId": "d82492b6-0c7e-4799-8fbd-4b2c7e875cb6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Ridge Regression ...\n",
      "Training Lasso Regression ...\n",
      "--------------------------------------------------\n",
      "Final Validation Loss of Ridge Regression: 27.616\n",
      "Final Validation Loss of Lasso Regression: 27.759\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### SOLUTION ###\n",
    "import pickle\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Change SALIENT to False to print the intermediate losses during training\n",
    "SALIENT = True\n",
    "\n",
    "np.random.seed(42) \n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def permute_data(x, y):\n",
    "    if y.ndim == 1:\n",
    "        y = np.expand_dims(y, axis=-1)\n",
    "    cat_xy = np.concatenate((x, y), axis=-1)\n",
    "    cat_xy = np.random.permutation(cat_xy)\n",
    "    return cat_xy[:, :-1], cat_xy[:, -1]\n",
    "\n",
    "\n",
    "def l2_norm_square(x):\n",
    "    return np.sum(np.power(x, 2))\n",
    "\n",
    "\n",
    "def mse(X, y, w):\n",
    "    return l2_norm_square(np.matmul(X, w) - y)\n",
    "\n",
    "\n",
    "def Ridge_loss(X, y, w, LAMBDA):\n",
    "    \"\"\"\n",
    "    X: n x d matrix\n",
    "    y: column vector of size (n,)\n",
    "    w: column vector of size (d, )\n",
    "    LAMBDA: a float scalar\n",
    "    Return: a float scalar showing ridge regression loss with the given X, y, w, LAMBDA\n",
    "    \"\"\"\n",
    "    j = mse(X, y, w) + LAMBDA * l2_norm_square(w)\n",
    "    return j\n",
    "\n",
    "\n",
    "def Ridge_grad(X, y, w, LAMBDA):\n",
    "    \"\"\"\n",
    "    X: n x d matrix\n",
    "    y: column vector of size (n,)\n",
    "    w: column vector of size (d, )\n",
    "    LAMBDA: a float scalar\n",
    "    Return: a column vector of size (d,) showing gradient of ridge regression loss with the given X, y, w, LAMBDA\n",
    "\n",
    "    \"\"\"\n",
    "    y_pred = np.matmul(X, w)\n",
    "    mse_grad = 2 * np.matmul(np.transpose(X), y_pred - y)\n",
    "    regularizer_grad = 2 * LAMBDA * w\n",
    "    return mse_grad + regularizer_grad\n",
    "\n",
    "\n",
    "def Lasso_loss(X, y, w, LAMBDA):\n",
    "    \"\"\"\n",
    "    X: n x d matrix\n",
    "    y: column vector of size (n,)\n",
    "    w: column vector of size (d, )\n",
    "    LAMBDA: a float scalar\n",
    "    Return: a float scalar showing Lasso regression loss with the given X, y, w, LAMBDA\n",
    "    \"\"\"\n",
    "    j = mse(X, y, w) + LAMBDA * np.abs(w).sum()\n",
    "    return j\n",
    "\n",
    "\n",
    "def Lasso_grad(X, y, w, LAMBDA):\n",
    "    \"\"\"\n",
    "    X: n x d matrix\n",
    "    y: column vector of size (n,)\n",
    "    w: column vector of size (d, )\n",
    "    LAMBDA: a float scalar\n",
    "    Return: a column vector of size (d,) showing gradient of lasso regression loss with the given X, y, w, LAMBDA\n",
    "    \"\"\"\n",
    "    y_pred = np.matmul(X, w)\n",
    "    mse_grad = 2 * np.matmul(np.transpose(X), y_pred - y)\n",
    "    regularizer_grad = LAMBDA * np.sign(w)\n",
    "    return mse_grad + regularizer_grad\n",
    "\n",
    "\n",
    "def training(X_train, y_train, X_validation, y_validation, w, LAMBDA, STEP_SIZE, method='Ridge'):\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_TRAIN = len(X_train)\n",
    "    X_p, y_p = permute_data(X_train, y_train)\n",
    "    if method == 'Ridge':\n",
    "        loss_fn = Ridge_loss\n",
    "        grad_fn = Ridge_grad\n",
    "    elif method == 'Lasso':\n",
    "        loss_fn = Lasso_loss\n",
    "        grad_fn = Lasso_grad\n",
    "    else:\n",
    "        raise ValueError('method = %s is not implemented' % method)\n",
    "\n",
    "    # train\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_loss = AverageMeter()\n",
    "        for i in range(0, NUM_TRAIN, BATCH_SIZE):\n",
    "            X_batch = X_p[i: min(i + BATCH_SIZE, NUM_TRAIN)]\n",
    "            y_batch = y_p[i: min(i + BATCH_SIZE, NUM_TRAIN)]\n",
    "\n",
    "            loss = loss_fn(X_batch, y_batch, w, LAMBDA)\n",
    "            epoch_loss.update(loss)\n",
    "            if i % 100 == 0 and (not SALIENT):\n",
    "                print('Epoch [%d/%d] Train: Current Train Loss: %.3f (Avg Loss %.3f)' % (\n",
    "                epoch, NUM_EPOCHS, loss, epoch_loss.avg))\n",
    "            grad = grad_fn(X_batch, y_batch, w, LAMBDA)\n",
    "            w -= STEP_SIZE * grad\n",
    "        val_loss = validation(X_validation, y_validation, w)\n",
    "        if not SALIENT:\n",
    "            print('Epoch [%d/%d] Val: Validation Loss: %.3f' % (epoch, NUM_EPOCHS, val_loss))\n",
    "        X_p, y_p = permute_data(X_train, y_train)\n",
    "\n",
    "    # val\n",
    "    val_loss = validation(X_validation, y_validation, w)\n",
    "    if not SALIENT:\n",
    "        print('Final Validation Loss of %s Regression: %.3f' % (method, val_loss))\n",
    "    return val_loss, w\n",
    "\n",
    "\n",
    "def validation(X, y, w):\n",
    "    val_loss = mse(X, y, w)\n",
    "    return val_loss\n",
    "\n",
    "file_name = \"lasso_data_ours.pickle\"\n",
    "with open(file_name, 'rb') as f_myfile:\n",
    "    data = pickle.load(f_myfile)\n",
    "    f_myfile.close()\n",
    "\n",
    "X_train, y_train, X_validation, y_validation = data['x_train'], data['y_train'], data['x_test'], data['y_test']\n",
    "\n",
    "## Train and test Ridge Regression\n",
    "\n",
    "NUM_EPOCHS = 1000\n",
    "LAMBDA_R = 1\n",
    "LAMBDA_L = 0.5\n",
    "BATCH_SIZE = 16\n",
    "STEP_SIZE = 0.0001\n",
    "FEAT_SIZE = X_train.shape[1]\n",
    "INIT_W = np.random.randn(FEAT_SIZE)\n",
    "print('Training Ridge Regression ...')\n",
    "loss_ridge,  w_r = training(X_train, y_train, X_validation, y_validation, deepcopy(INIT_W), LAMBDA_R, STEP_SIZE, method='Ridge')\n",
    "print('Training Lasso Regression ...')\n",
    "loss_lasso,  w_l = training(X_train, y_train, X_validation, y_validation, deepcopy(INIT_W), LAMBDA_L, STEP_SIZE, method='Lasso')\n",
    "print('-' * 50)\n",
    "print('Final Validation Loss of Ridge Regression: %.3f' % loss_ridge)\n",
    "print('Final Validation Loss of Lasso Regression: %.3f' % loss_lasso)\n",
    "\n",
    "# Visualize the learned weight via Ridge Regression and Lasso Regression\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "ax[0].stem(list(range(FEAT_SIZE)), w_r)\n",
    "ax[0].grid()\n",
    "ax[0].set_title('Ridge Regression')\n",
    "ax[0].set_ylabel('w')\n",
    "\n",
    "\n",
    "ax[1].stem(list(range(FEAT_SIZE)), w_l)\n",
    "ax[1].grid()\n",
    "ax[1].set_title('Lasso Regression')\n",
    "ax[1].set_ylabel('w')\n",
    "\n",
    "fig.set_size_inches(15, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1E8Z7B1MlK1"
   },
   "source": [
    "By comparing the learned $\\mathbf{w}$ from Ridge Regression and Lasso Regression, we can find Lasso Regression produces sparse $\\mathbf{w}$ via L1 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tz_dW2GdT4K6"
   },
   "source": [
    "**Q4.3**: Hyper-parameter Tuning. Please use different $\\lambda_l$ in the Lasso Regression to find out the best $\\lambda_l$ with the loss on the validation set.  Specifically, you should use the validation set to determine the best value for    and its corresponding validation error/loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "uOgWe22xIv-2",
    "outputId": "bdf8c600-4019-4bbc-ec99-da48eb6d2898"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The best lambda_l is 0.500 and its corresponding validation loss is 27.925\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### SOLUTION ###\n",
    "# Try different lambdas for Lasso Regression\n",
    "LAMBDA_Ls = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10]\n",
    "losses = []\n",
    "best_lambda_l, best_validation_loss = None, 1e9\n",
    "for LAMBDA_L in LAMBDA_Ls:\n",
    "    loss_lasso, w_l = training(X_train, y_train, X_validation, y_validation, deepcopy(INIT_W), LAMBDA_L, STEP_SIZE,\n",
    "                           method='Lasso')\n",
    "    if loss_lasso < best_validation_loss:\n",
    "      best_lambda_l = LAMBDA_L\n",
    "      best_validation_loss = loss_lasso\n",
    "    losses.append(loss_lasso)\n",
    "\n",
    "print(' The best lambda_l is %.3f and its corresponding validation loss is %.3f' % (best_lambda_l, best_validation_loss))\n",
    "\n",
    "plt.plot(LAMBDA_Ls, losses)\n",
    "plt.xlabel('lambda_l')\n",
    "plt.ylabel('validation loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "HW1_Solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
