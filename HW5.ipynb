{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 5 - Reinforcement Learning and Transformers \n",
    "\n",
    "<h4> Reinforcement Learning Section by Sid Mysore. <br> Transformers Section adapted from previous homework designed by Ruizhao Zhu with help of Brian Kulis and Ashok Cutkosky<br> </h4>\n",
    "\n",
    "---\n",
    "\n",
    "This assignment is broken up into 2 parts:\n",
    "1. Implementing and testing basic deep RL algorithms for:\n",
    "    * Discrete action spaces with DQN (simplified from the seminal Nature paper)\n",
    "    * Continuous action spaces with DDPG (a popular basic actor-critic algorithm)\n",
    "2. Understanding the basic structure of Transformers\n",
    "\n",
    "This code has been tested locally on Linux, Windows 10 and MacOS, and on Colab\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes before running code\n",
    "\n",
    "1. If running on Windows, note that some packages may throw warnings or errors - if you encounter this, try the fixes recommended by the error message(s)\n",
    "\n",
    "2. Additional packages may be required for running locally. This notebook is configured to help you install specific versions of the required packages but if you are concerned about them overwriting exisiting configs for other projects, it is recommended to create a new python environment.\n",
    "\n",
    "2. When running code, make sure the following files are in your current working directory as they will be needed for some of the RL logging and plotting:\n",
    "    * `test_policy.py`\n",
    "    * `logx.py`\n",
    "    * `plot.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Deep Q Learning (30 Points)\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "<img src=\"http://ai.bu.edu/DL523/HW5_files/rl_diagram.png\" width=\"360em\">\n",
    "\n",
    "The main components of the RL optimization loop are the agent and the environment. The environment is the world that the agent interacts with. At every step of interaction, the agent sees a (possibly partial) observation of the state of the world, and then decides on an action to take. The environment may change as a response to the agents' actions on it, but may also change on its own.\n",
    "\n",
    "The agent also perceives a reward signal from the environment, a number that tells it how good or bad the current world state is. The goal of the agent is to maximize its cumulative reward. Reinforcement learning methods are ways that the agent can learn behaviors to achieve its goal.\n",
    "\n",
    "##### Fully vs. Partially Observable State-spaces\n",
    "\n",
    "When the agent is able to observe the complete state of the environment, we say that the environment is fully observed. When the agent can only see a partial observation, we say that the environment is partially observed. For the purposes of this homework, we will be dealing with fully observable environments.\n",
    "\n",
    "##### Discrete vs. Continuous Action-spaces\n",
    "\n",
    "Different environments allow different kinds of actions. The set of all valid actions in a given environment is often called the action space. Some environments, like old Atari games and Go, have discrete action spaces, where only a finite number of moves are available to the agent. Other environments, like where the agent controls a robot in a physical world, have continuous action spaces. In continuous spaces, actions are real-valued vectors. In this homework, we will consider problems involving both types of action spaces and some simple algorithms for solving them.\n",
    "\n",
    "##### The RL Problem\n",
    "\n",
    "The reward function R is critically important in reinforcement learning. It depends on the current state of the world, the action just taken, and the next state of the world:\n",
    "\n",
    "$r_t = R(s_t, a_t, s_{t+1})$\n",
    "\n",
    "although frequently this is simplified to just a dependence on the current state, $r_t = R(s_t)$, or state-action pair $r_t = R(s_t,a_t)$.\n",
    "\n",
    "The goal of the agent is to maximize the expected cumulative reward over a trajectory, $\\tau$, but this actually can mean a few things. Weâ€™ll notate all of these cases with $R(\\tau)$. The expected return, denoted by $J(\\pi)$, is then:\n",
    "\n",
    "$J(\\pi) = \\mathbb{E}_{\\tau\\sim \\pi}{R(\\tau)}$\n",
    "\n",
    "The central optimization problem in RL can then be expressed by\n",
    "\n",
    "$\\pi^* = \\arg \\max_{\\pi} J(\\pi)$\n",
    "\n",
    "with $\\pi^*$ being the optimal policy.\n",
    "\n",
    "##### The need to approximate value functions\n",
    "\n",
    "A way to frame the RL problem would be to say that we want actions taken by our agents to maximize the expected achievable value, given a starting state. This can be in the form of a state-value function:\n",
    "\n",
    "$V^{\\pi}(s) = \\mathbb{E}_{\\tau \\sim \\pi}[R(\\tau) | s_0 = s]$ for states $s \\in S$\n",
    "\n",
    "or a state-action value function (often called a Q-function):\n",
    "\n",
    "$Q^{\\pi}(s,a) = \\mathbb{E}_{\\tau \\sim \\pi}[R(\\tau) | s_0 = s, a_0 = a]$ for states $s \\in S$ and actions $a \\in A$\n",
    "\n",
    "Both value representations obey self-consistency equations called Bellman equations, of which the basic idea is that:\n",
    "\n",
    "    The value of your starting point is the reward you expect to get from being there, plus the value of wherever you land next.\n",
    "\n",
    "For a given policy, $\\pi$, the Bellman equations are:\n",
    "\n",
    "$ V^{\\pi}(s_t) = \\mathbb{E}_{a \\sim \\pi}[r(s_t,a_t) + \\gamma V^\\pi(s_{t+1})]$\n",
    "\n",
    "$ Q^{\\pi}(s_t, a_t) = \\mathbb{E}_{s}[r(s_t,a_t) + \\gamma Q^\\pi(s_{t+1},\\pi(s_{t+1}))]]$\n",
    "\n",
    "The optimal policy is one that maximizes the expected values $V$ and/or $Q$. This homework will mainly focus on Q-learning techniques, so we are mainly concerned with techniques that consider the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Setup\n",
    "\n",
    "Now for some setup to use during the RL parts of this homework.\n",
    "\n",
    "We will be representing policy and value functions by neural networks. The problems we're working on are fairly simple so we'll be using some simple multi-layer perceptrons (MLPs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below if you're starting from a clean environment or suspect you may have the wrong versions of packages \n",
    "\n",
    "*(Also assumes the packages `ipython` and `jupyter` are already installed since you need them anyway to use this notebook)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from logx import EpochLogger\n",
    "from test_policy import load_policy_and_env, run_policy\n",
    "from plot import plot_data, get_datasets\n",
    "\n",
    "def combined_shape(length, shape=None):\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use replay buffers to store trajectories during rollouts so that we can sample from them to aid in training. By storing experiences in replay buffers and sampling from them, we can mitigate catastrophic forgetting (when RL agents forget old experiences).\n",
    "\n",
    "The Replay Buffer mainly keeps a store of the core elements of the Markov Decision Process (MDP) tuple for each interaction of the agent in the training environment. This includes the observed state (obs), the action taken (act), the reward received (rew), the resultant next state after the action (next_obs) and whether the episode was completed/terminated (done).\n",
    "\n",
    "This data can then be sampled during optimization to use for batch updates, as we will do in this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A simple FIFO experience replay buffer\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_dim, act_dim, size, discrete=False):\n",
    "        self.discrete=discrete\n",
    "        self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr+1) % self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        batch = dict(obs=self.obs_buf[idxs],\n",
    "                     obs2=self.obs2_buf[idxs],\n",
    "                     act=self.act_buf[idxs],\n",
    "                     rew=self.rew_buf[idxs],\n",
    "                     done=self.done_buf[idxs])\n",
    "        return {k: torch.as_tensor(v, dtype=torch.int64 if (k=='act' and self.discrete) else torch.float32) for k,v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1 Deep Q Networks (DQNs)\n",
    "\n",
    "The main idea behind Q-learning is that if we had a function $Q^*:StateÃ—Action \\rightarrow \\mathbb{R}$, that could tell us what our return would be, if we were to take an action in a given state, then we could easily construct a policy that maximizes our rewards:\n",
    "\n",
    "$\\pi^*(s_t) = \\argmax_a Q^*(s_t,a)$\n",
    "\n",
    "However, we donâ€™t know everything about the world, so we donâ€™t have access to $Q^*$. But, since neural networks are universal function approximators, we can simply create one and train it to resemble $Q^*$.\n",
    "\n",
    "For a discrete action space, a Q-network can be used to estimate the Q-values for each possible action, given a state. The policy can then be designed such that:\n",
    "\n",
    "$a^\\pi = \\argmax_a Q^\\pi(s_t,a)$\n",
    "\n",
    "For our training update rule, weâ€™ll use a fact that every $Q$ function for some policy obeys the Bellman equation:\n",
    "\n",
    "$Q^\\pi(s_t,a) = r + \\gamma Q^\\pi(s_{t+1},\\pi(s_{t+1}))$\n",
    "\n",
    "The difference between the two sides of the equality is known as the temporal difference error\n",
    "\n",
    "$\\delta = Q(s_{t},a) - (r + \\gamma \\max_a Q(s_{t+1}, a))$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_space, act_space, hidden_sizes=(64,64), activation=nn.ReLU, output_activation=nn.Identity):\n",
    "        super().__init__()\n",
    "        h_sizes = [obs_space.shape[0]] + list(hidden_sizes) + [act_space.n]\n",
    "        self.Q = mlp(h_sizes, activation, output_activation)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Return output from network scaled to action space limits.\n",
    "        return self.Q(obs)\n",
    "\n",
    "    def act(self, obs):\n",
    "        with torch.no_grad():\n",
    "            return self.Q(obs[None,]).max(1)[1].numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the above simple DQN structure to train a policy to solve OpenAI gym's CartPole problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "steps_per_epoch=4000\n",
    "epochs=20\n",
    "gamma=0.99\n",
    "polyak=0.99  \n",
    "q_lr=1e-3\n",
    "batch_size=100\n",
    "start_steps=10000\n",
    "update_after=1000 \n",
    "update_every=50\n",
    "act_noise=0.1\n",
    "num_test_episodes=10\n",
    "max_ep_len=1000\n",
    "save_freq=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also define a location for the trained models and logs to get saved. By default, it'll be stored in the current working directory but you may change this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Log dir dqn already exists! Storing info there anyway.\n",
      "\u001b[32;1mLogging data to dqn\\progress.txt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Define the directory location to save dqn logs and models\n",
    "dqn_output_dir = 'dqn'\n",
    "\n",
    "# Logger setup\n",
    "logger_kwargs={'output_dir':dqn_output_dir, 'exp_name':'dqn_CartPole'}\n",
    "logger = EpochLogger(**logger_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we'll be working on for DQN is the [CartPole](https://gym.openai.com/envs/CartPole-v1/) OpenAI Gym task. \n",
    "\n",
    "The agent observes the positions and velocities of the cart and pole and can take one of two discrete options to either move the cart to the left, or the right, to try and keep the pole balanced.\n",
    "\n",
    "The episode keeps going for up to 200 steps so long as the pole doesn't fall over too much, and the agent fails if the pole falls more than $\\pm 12$ degrees from the vertical. \n",
    "\n",
    "For every step that it remains 'alive', the agent gets 1 point of reward. The objective is to maximize this reward, which is equivalent to keep the pole balanced for as long as possible.\n",
    "\n",
    "Additional details for the implementation of this environment can be found via the code [source](https://github.com/openai/gym/blob/4ede9280f9c477f1ca09929d10cdc1e1ba1129f1/gym/envs/classic_control/cartpole.py).\n",
    "\n",
    "A random agent would quickly fail to balance the pole, as below (the frame hitches are due to the agent failing and the episode being reset)\n",
    "\n",
    "<img src=\"http://ai.bu.edu/DL523/HW5_files/cartpole_random_demo.gif\" width=\"360em\">\n",
    "\n",
    "But a trained agent can successfully solve this problem (using solution code for this HW).\n",
    "\n",
    "<img src=\"http://ai.bu.edu/DL523/HW5_files/cartpole_demo.gif\" width=\"360em\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Definition\n",
    "env_fn = lambda :gym.make('CartPole-v0')\n",
    "env, test_env = env_fn(), env_fn()\n",
    "obs_dim = env.observation_space.shape\n",
    "act_dim = env.action_space.shape\n",
    "\n",
    "# Seeding\n",
    "seed=0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "test_env.seed(seed)\n",
    "\n",
    "# Experience buffer\n",
    "replay_size=int(1e6) \n",
    "replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size, discrete=True)\n",
    "\n",
    "# Create DQN module\n",
    "Qnet = DQN(env.observation_space, env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stabilize training we also use a 'target' Q-network which is held relatively constant and is periodically updated with weights from the DQN being trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target network\n",
    "Qtarg = deepcopy(Qnet)\n",
    "\n",
    "# Freeze target networks with respect to optimizers (only update via polyak averaging)\n",
    "for p in Qtarg.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Set up model saving\n",
    "logger.setup_pytorch_saver(Qnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can next initialize some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def get_action(o, eps_thresh):\n",
    "    a = Qnet.act(torch.as_tensor(o, dtype=torch.float32))\n",
    "    if np.random.rand() < eps_thresh:\n",
    "        a = env.action_space.sample()\n",
    "    return a\n",
    "\n",
    "def test_agent():\n",
    "    for j in range(num_test_episodes):\n",
    "        o, d, ep_ret, ep_len = test_env.reset(), False, 0, 0\n",
    "        while not(d or (ep_len == max_ep_len)):\n",
    "            # Take deterministic actions at test time (noise_scale=0)\n",
    "            o, r, d, _ = test_env.step(get_action(o, 0))\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "        logger.store(TestEpRet=ep_ret, TestEpLen=ep_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define DQN Q-learning Loss [10 points]\n",
    "\n",
    "For a sampled batch of data from the replay buffer consisting of sets of observations (o), corresponding actions taken (a), achieved reward (r), resultant observations (o2), and a flag indicating if the recorded step solved the problem or ended the episode (d), we compute the DQN loss in 4 main steps:\n",
    "\n",
    "1. Compute the Q-network output for o and extract Q-values for specific actions taken, a: $Q(s,a)$ - Actions, $a \\in [0,n-1]$, are indices into a discrete action space with $n$ possible actions.\n",
    "2. Next, compute the Bellman backup: $b = r + \\gamma \\max_a Q_{targ}(s,a)$ - remember that $\\gamma = 0$ if $d=1$ and that you don't need to track gradients through the target Q-network.\n",
    "3. Compute the Bellman error: $e = Q(s,a) - b$\n",
    "4. Finally compute the Q-loss: $l_Q = \\frac{1}{N} || e ||_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up function for computing DQN Q-loss\n",
    "def compute_loss_q(data):\n",
    "    # Batch data sample:\n",
    "    o, a, r, o2, d = data['obs'], data['act'], data['rew'], data['obs2'], data['done']\n",
    "#     print(o.size(),a.size(),r.size(),o2.size(),d.size())\n",
    "    \"\"\"\n",
    "    torch.Size([100, 4]) torch.Size([100]) torch.Size([100]) torch.Size([100, 4]) torch.Size([100])\n",
    "    \"\"\"\n",
    "    \"\"\" STUDENT CODE GOES HERE \"\"\"\n",
    "    q = torch.gather(Qnet(o),dim =1, index=a.view(-1,1).long()) # Compute q values for observations as appropriate\n",
    "    with torch.no_grad():\n",
    "        q_tar = torch.max(Qtarg(o2),dim =1).values\n",
    "        b = r + (1-d) * gamma * q_tar\n",
    "    loss_q = nn.functional.smooth_l1_loss(q[:,0],b)\n",
    "\n",
    "    \"\"\" STUDENT CODE ENDS \"\"\" \n",
    "\n",
    "    # Useful info for logging\n",
    "    loss_info = dict(QVals=q.detach().numpy())\n",
    "\n",
    "    return loss_q, loss_info\n",
    "\n",
    "# Set up optimizers for policy and q-function\n",
    "q_optimizer = Adam(Qnet.Q.parameters(), lr=q_lr)\n",
    "\n",
    "def update(data):\n",
    "    # First run one gradient descent step for Q.\n",
    "    q_optimizer.zero_grad()\n",
    "    loss_q, loss_info = compute_loss_q(data)\n",
    "    loss_q.backward()\n",
    "    q_optimizer.step()\n",
    "\n",
    "    # Record things\n",
    "    logger.store(LossQ=loss_q.item(), **loss_info)\n",
    "\n",
    "    # Finally, update target networks by polyak averaging.\n",
    "    with torch.no_grad():\n",
    "        for p, p_targ in zip(Qnet.parameters(), Qtarg.parameters()):\n",
    "            # NB: We use an in-place operations \"mul_\", \"add_\" to update target\n",
    "            # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
    "            p_targ.data.mul_(polyak)\n",
    "            p_targ.data.add_((1 - polyak) * p.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run training.\n",
    "\n",
    "While running the code, pay attention the following logged information to determine if things are progressing well:\n",
    "\n",
    "1. For the first few thousand steps, the agents act randomly so you'll likely see low rewards that don't improve until more than `start_steps` steps have elapsed.\n",
    "2. Average Episode and Test Episode returns should generally be increasing (up to 200 which is the max for the CartPole problem)\n",
    "3. Despite optimizing the Q-value loss, the fact that we're having the network learn an arbitrary value means that we may never hit a loss of 0 - the loss should stabilize after some time though (feel free to experiment with more epochs of training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "|             Epoch |               1 |\n",
      "|      AverageEpRet |            22.7 |\n",
      "|          StdEpRet |            11.7 |\n",
      "|          MaxEpRet |              76 |\n",
      "|          MinEpRet |               9 |\n",
      "|  AverageTestEpRet |             137 |\n",
      "|      StdTestEpRet |            29.9 |\n",
      "|      MaxTestEpRet |             180 |\n",
      "|      MinTestEpRet |              90 |\n",
      "|             EpLen |            22.7 |\n",
      "|         TestEpLen |             137 |\n",
      "| TotalEnvInteracts |           4e+03 |\n",
      "|      AverageQVals |            13.5 |\n",
      "|          StdQVals |            8.26 |\n",
      "|          MaxQVals |            32.1 |\n",
      "|          MinQVals |           -4.19 |\n",
      "|             LossQ |           0.251 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |               2 |\n",
      "|      AverageEpRet |              24 |\n",
      "|          StdEpRet |            12.5 |\n",
      "|          MaxEpRet |              81 |\n",
      "|          MinEpRet |               8 |\n",
      "|  AverageTestEpRet |            91.1 |\n",
      "|      StdTestEpRet |            4.11 |\n",
      "|      MaxTestEpRet |              99 |\n",
      "|      MinTestEpRet |              85 |\n",
      "|             EpLen |              24 |\n",
      "|         TestEpLen |            91.1 |\n",
      "| TotalEnvInteracts |           8e+03 |\n",
      "|      AverageQVals |            43.2 |\n",
      "|          StdQVals |            14.3 |\n",
      "|          MaxQVals |            73.8 |\n",
      "|          MinQVals |           -33.4 |\n",
      "|             LossQ |           0.648 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |               3 |\n",
      "|      AverageEpRet |            26.7 |\n",
      "|          StdEpRet |            27.6 |\n",
      "|          MaxEpRet |             200 |\n",
      "|          MinEpRet |              10 |\n",
      "|  AverageTestEpRet |             132 |\n",
      "|      StdTestEpRet |            7.38 |\n",
      "|      MaxTestEpRet |             148 |\n",
      "|      MinTestEpRet |             122 |\n",
      "|             EpLen |            26.7 |\n",
      "|         TestEpLen |             132 |\n",
      "| TotalEnvInteracts |         1.2e+04 |\n",
      "|      AverageQVals |            69.4 |\n",
      "|          StdQVals |            19.6 |\n",
      "|          MaxQVals |             152 |\n",
      "|          MinQVals |           -45.1 |\n",
      "|             LossQ |           0.913 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |               4 |\n",
      "|      AverageEpRet |            52.5 |\n",
      "|          StdEpRet |            44.9 |\n",
      "|          MaxEpRet |             200 |\n",
      "|          MinEpRet |              10 |\n",
      "|  AverageTestEpRet |            63.8 |\n",
      "|      StdTestEpRet |            6.16 |\n",
      "|      MaxTestEpRet |              73 |\n",
      "|      MinTestEpRet |              56 |\n",
      "|             EpLen |            52.5 |\n",
      "|         TestEpLen |            63.8 |\n",
      "| TotalEnvInteracts |         1.6e+04 |\n",
      "|      AverageQVals |            91.4 |\n",
      "|          StdQVals |            25.6 |\n",
      "|          MaxQVals |             191 |\n",
      "|          MinQVals |           -59.8 |\n",
      "|             LossQ |            1.34 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |               5 |\n",
      "|      AverageEpRet |              42 |\n",
      "|          StdEpRet |            26.1 |\n",
      "|          MaxEpRet |              96 |\n",
      "|          MinEpRet |               9 |\n",
      "|  AverageTestEpRet |            81.6 |\n",
      "|      StdTestEpRet |            22.3 |\n",
      "|      MaxTestEpRet |              92 |\n",
      "|      MinTestEpRet |              15 |\n",
      "|             EpLen |              42 |\n",
      "|         TestEpLen |            81.6 |\n",
      "| TotalEnvInteracts |           2e+04 |\n",
      "|      AverageQVals |             114 |\n",
      "|          StdQVals |            31.6 |\n",
      "|          MaxQVals |             232 |\n",
      "|          MinQVals |           -70.7 |\n",
      "|             LossQ |            1.96 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |               6 |\n",
      "|      AverageEpRet |            72.8 |\n",
      "|          StdEpRet |            32.8 |\n",
      "|          MaxEpRet |             102 |\n",
      "|          MinEpRet |              10 |\n",
      "|  AverageTestEpRet |            95.1 |\n",
      "|      StdTestEpRet |            3.21 |\n",
      "|      MaxTestEpRet |             103 |\n",
      "|      MinTestEpRet |              91 |\n",
      "|             EpLen |            72.8 |\n",
      "|         TestEpLen |            95.1 |\n",
      "| TotalEnvInteracts |         2.4e+04 |\n",
      "|      AverageQVals |             144 |\n",
      "|          StdQVals |            39.2 |\n",
      "|          MaxQVals |             237 |\n",
      "|          MinQVals |             -89 |\n",
      "|             LossQ |            2.54 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |               7 |\n",
      "|      AverageEpRet |            57.2 |\n",
      "|          StdEpRet |            34.8 |\n",
      "|          MaxEpRet |             101 |\n",
      "|          MinEpRet |              11 |\n",
      "|  AverageTestEpRet |            57.8 |\n",
      "|      StdTestEpRet |            2.96 |\n",
      "|      MaxTestEpRet |              63 |\n",
      "|      MinTestEpRet |              52 |\n",
      "|             EpLen |            57.2 |\n",
      "|         TestEpLen |            57.8 |\n",
      "| TotalEnvInteracts |         2.8e+04 |\n",
      "|      AverageQVals |             160 |\n",
      "|          StdQVals |              35 |\n",
      "|          MaxQVals |             211 |\n",
      "|          MinQVals |           -94.6 |\n",
      "|             LossQ |            2.55 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |               8 |\n",
      "|      AverageEpRet |             136 |\n",
      "|          StdEpRet |            58.3 |\n",
      "|          MaxEpRet |             200 |\n",
      "|          MinEpRet |              11 |\n",
      "|  AverageTestEpRet |             200 |\n",
      "|      StdTestEpRet |               0 |\n",
      "|      MaxTestEpRet |             200 |\n",
      "|      MinTestEpRet |             200 |\n",
      "|             EpLen |             136 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |         3.2e+04 |\n",
      "|      AverageQVals |             149 |\n",
      "|          StdQVals |            31.2 |\n",
      "|          MaxQVals |             180 |\n",
      "|          MinQVals |           -96.9 |\n",
      "|             LossQ |            2.04 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |               9 |\n",
      "|      AverageEpRet |             200 |\n",
      "|          StdEpRet |               0 |\n",
      "|          MaxEpRet |             200 |\n",
      "|          MinEpRet |             200 |\n",
      "|  AverageTestEpRet |             200 |\n",
      "|      StdTestEpRet |               0 |\n",
      "|      MaxTestEpRet |             200 |\n",
      "|      MinTestEpRet |             200 |\n",
      "|             EpLen |             200 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |         3.6e+04 |\n",
      "|      AverageQVals |             130 |\n",
      "|          StdQVals |            31.3 |\n",
      "|          MaxQVals |             160 |\n",
      "|          MinQVals |           -88.3 |\n",
      "|             LossQ |            1.45 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              10 |\n",
      "|      AverageEpRet |             167 |\n",
      "|          StdEpRet |            58.9 |\n",
      "|          MaxEpRet |             200 |\n",
      "|          MinEpRet |              16 |\n",
      "|  AverageTestEpRet |             200 |\n",
      "|      StdTestEpRet |               0 |\n",
      "|      MaxTestEpRet |             200 |\n",
      "|      MinTestEpRet |             200 |\n",
      "|             EpLen |             167 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |           4e+04 |\n",
      "|      AverageQVals |             119 |\n",
      "|          StdQVals |            37.1 |\n",
      "|          MaxQVals |             148 |\n",
      "|          MinQVals |           -80.3 |\n",
      "|             LossQ |            1.17 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              11 |\n",
      "|      AverageEpRet |            96.6 |\n",
      "|          StdEpRet |            67.1 |\n",
      "|          MaxEpRet |             200 |\n",
      "|          MinEpRet |              14 |\n",
      "|  AverageTestEpRet |            63.4 |\n",
      "|      StdTestEpRet |            43.5 |\n",
      "|      MaxTestEpRet |             111 |\n",
      "|      MinTestEpRet |              16 |\n",
      "|             EpLen |            96.6 |\n",
      "|         TestEpLen |            63.4 |\n",
      "| TotalEnvInteracts |         4.4e+04 |\n",
      "|      AverageQVals |             129 |\n",
      "|          StdQVals |            35.8 |\n",
      "|          MaxQVals |             233 |\n",
      "|          MinQVals |           -76.9 |\n",
      "|             LossQ |             1.6 |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "|             Epoch |              12 |\n",
      "|      AverageEpRet |              50 |\n",
      "|          StdEpRet |            43.5 |\n",
      "|          MaxEpRet |             163 |\n",
      "|          MinEpRet |              10 |\n",
      "|  AverageTestEpRet |             181 |\n",
      "|      StdTestEpRet |            3.54 |\n",
      "|      MaxTestEpRet |             184 |\n",
      "|      MinTestEpRet |             173 |\n",
      "|             EpLen |              50 |\n",
      "|         TestEpLen |             181 |\n",
      "| TotalEnvInteracts |         4.8e+04 |\n",
      "|      AverageQVals |             159 |\n",
      "|          StdQVals |            40.7 |\n",
      "|          MaxQVals |             234 |\n",
      "|          MinQVals |            -134 |\n",
      "|             LossQ |            2.14 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              13 |\n",
      "|      AverageEpRet |             199 |\n",
      "|          StdEpRet |            6.32 |\n",
      "|          MaxEpRet |             200 |\n",
      "|          MinEpRet |             171 |\n",
      "|  AverageTestEpRet |             200 |\n",
      "|      StdTestEpRet |               0 |\n",
      "|      MaxTestEpRet |             200 |\n",
      "|      MinTestEpRet |             200 |\n",
      "|             EpLen |             199 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |         5.2e+04 |\n",
      "|      AverageQVals |             170 |\n",
      "|          StdQVals |              39 |\n",
      "|          MaxQVals |             260 |\n",
      "|          MinQVals |             -99 |\n",
      "|             LossQ |            2.34 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              14 |\n",
      "|      AverageEpRet |            67.8 |\n",
      "|          StdEpRet |            71.6 |\n",
      "|          MaxEpRet |             200 |\n",
      "|          MinEpRet |              11 |\n",
      "|  AverageTestEpRet |             120 |\n",
      "|      StdTestEpRet |            3.94 |\n",
      "|      MaxTestEpRet |             126 |\n",
      "|      MinTestEpRet |             114 |\n",
      "|             EpLen |            67.8 |\n",
      "|         TestEpLen |             120 |\n",
      "| TotalEnvInteracts |         5.6e+04 |\n",
      "|      AverageQVals |             169 |\n",
      "|          StdQVals |            29.2 |\n",
      "|          MaxQVals |             314 |\n",
      "|          MinQVals |           -50.7 |\n",
      "|             LossQ |            2.18 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              15 |\n",
      "|      AverageEpRet |            82.8 |\n",
      "|          StdEpRet |            42.9 |\n",
      "|          MaxEpRet |             172 |\n",
      "|          MinEpRet |               9 |\n",
      "|  AverageTestEpRet |             118 |\n",
      "|      StdTestEpRet |            6.97 |\n",
      "|      MaxTestEpRet |             133 |\n",
      "|      MinTestEpRet |             111 |\n",
      "|             EpLen |            82.8 |\n",
      "|         TestEpLen |             118 |\n",
      "| TotalEnvInteracts |           6e+04 |\n",
      "|      AverageQVals |             171 |\n",
      "|          StdQVals |              32 |\n",
      "|          MaxQVals |             312 |\n",
      "|          MinQVals |           -41.4 |\n",
      "|             LossQ |            2.33 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              16 |\n",
      "|      AverageEpRet |            84.9 |\n",
      "|          StdEpRet |            34.4 |\n",
      "|          MaxEpRet |             115 |\n",
      "|          MinEpRet |              11 |\n",
      "|  AverageTestEpRet |            79.3 |\n",
      "|      StdTestEpRet |              34 |\n",
      "|      MaxTestEpRet |             103 |\n",
      "|      MinTestEpRet |              25 |\n",
      "|             EpLen |            84.9 |\n",
      "|         TestEpLen |            79.3 |\n",
      "| TotalEnvInteracts |         6.4e+04 |\n",
      "|      AverageQVals |             181 |\n",
      "|          StdQVals |            32.7 |\n",
      "|          MaxQVals |             256 |\n",
      "|          MinQVals |           -52.3 |\n",
      "|             LossQ |            2.48 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              17 |\n",
      "|      AverageEpRet |             142 |\n",
      "|          StdEpRet |            53.2 |\n",
      "|          MaxEpRet |             200 |\n",
      "|          MinEpRet |              33 |\n",
      "|  AverageTestEpRet |            93.2 |\n",
      "|      StdTestEpRet |            1.78 |\n",
      "|      MaxTestEpRet |              96 |\n",
      "|      MinTestEpRet |              91 |\n",
      "|             EpLen |             142 |\n",
      "|         TestEpLen |            93.2 |\n",
      "| TotalEnvInteracts |         6.8e+04 |\n",
      "|      AverageQVals |             191 |\n",
      "|          StdQVals |            35.9 |\n",
      "|          MaxQVals |             274 |\n",
      "|          MinQVals |           -63.3 |\n",
      "|             LossQ |            2.78 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              18 |\n",
      "|      AverageEpRet |            69.7 |\n",
      "|          StdEpRet |            64.2 |\n",
      "|          MaxEpRet |             200 |\n",
      "|          MinEpRet |              10 |\n",
      "|  AverageTestEpRet |             200 |\n",
      "|      StdTestEpRet |               0 |\n",
      "|      MaxTestEpRet |             200 |\n",
      "|      MinTestEpRet |             200 |\n",
      "|             EpLen |            69.7 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |         7.2e+04 |\n",
      "|      AverageQVals |             195 |\n",
      "|          StdQVals |            35.3 |\n",
      "|          MaxQVals |             265 |\n",
      "|          MinQVals |           -47.9 |\n",
      "|             LossQ |            2.63 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              19 |\n",
      "|      AverageEpRet |            85.6 |\n",
      "|          StdEpRet |            70.2 |\n",
      "|          MaxEpRet |             200 |\n",
      "|          MinEpRet |              11 |\n",
      "|  AverageTestEpRet |             200 |\n",
      "|      StdTestEpRet |               0 |\n",
      "|      MaxTestEpRet |             200 |\n",
      "|      MinTestEpRet |             200 |\n",
      "|             EpLen |            85.6 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |         7.6e+04 |\n",
      "|      AverageQVals |             198 |\n",
      "|          StdQVals |            40.4 |\n",
      "|          MaxQVals |             297 |\n",
      "|          MinQVals |           -56.2 |\n",
      "|             LossQ |            2.85 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              20 |\n",
      "|      AverageEpRet |             173 |\n",
      "|          StdEpRet |            61.5 |\n",
      "|          MaxEpRet |             200 |\n",
      "|          MinEpRet |              12 |\n",
      "|  AverageTestEpRet |             200 |\n",
      "|      StdTestEpRet |               0 |\n",
      "|      MaxTestEpRet |             200 |\n",
      "|      MinTestEpRet |             200 |\n",
      "|             EpLen |             173 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |           8e+04 |\n",
      "|      AverageQVals |             230 |\n",
      "|          StdQVals |            41.5 |\n",
      "|          MaxQVals |             357 |\n",
      "|          MinQVals |           -60.6 |\n",
      "|             LossQ |            3.29 |\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Prepare for interaction with environment\n",
    "total_steps = steps_per_epoch * epochs\n",
    "o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "# Main loop: collect experience in env and update/log each epoch\n",
    "for t in range(total_steps):\n",
    "    \n",
    "    # Until start_steps have elapsed, randomly sample actions\n",
    "    # from a uniform distribution for better exploration. Afterwards, \n",
    "    # use the learned policy (with some noise, via act_noise). \n",
    "    if t > start_steps:\n",
    "        a = get_action(o, act_noise)\n",
    "    else:\n",
    "        a = env.action_space.sample()\n",
    "\n",
    "    # Step the env\n",
    "    o2, r, d, _ = env.step(a)\n",
    "    ep_ret += r\n",
    "    ep_len += 1\n",
    "\n",
    "    # Ignore the \"done\" signal if it comes from hitting the time\n",
    "    # horizon (that is, when it's an artificial terminal signal\n",
    "    # that isn't based on the agent's state)\n",
    "    d = False if ep_len==max_ep_len else d\n",
    "\n",
    "    # Store experience to replay buffer\n",
    "    replay_buffer.store(o, a, r, o2, d)\n",
    "\n",
    "    # Super critical, easy to overlook step: make sure to update \n",
    "    # most recent observation!\n",
    "    o = o2\n",
    "\n",
    "    # End of trajectory handling\n",
    "    if d or (ep_len == max_ep_len):\n",
    "        logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
    "        o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "    # Update handling\n",
    "    if t >= update_after and t % update_every == 0:\n",
    "        for _ in range(update_every):\n",
    "            batch = replay_buffer.sample_batch(batch_size)\n",
    "            update(data=batch)\n",
    "\n",
    "    # End of epoch handling\n",
    "    if (t+1) % steps_per_epoch == 0:\n",
    "        epoch = (t+1) // steps_per_epoch\n",
    "\n",
    "        # Save model\n",
    "        if (epoch % save_freq == 0) or (epoch == epochs):\n",
    "            logger.save_state({'env': env}, None)\n",
    "\n",
    "        # Test the performance of the deterministic version of the agent.\n",
    "        test_agent()\n",
    "\n",
    "        # Log info about epoch\n",
    "        logger.log_tabular('Epoch', epoch)\n",
    "        logger.log_tabular('EpRet', with_min_and_max=True)\n",
    "        logger.log_tabular('TestEpRet', with_min_and_max=True)\n",
    "        logger.log_tabular('EpLen', average_only=True)\n",
    "        logger.log_tabular('TestEpLen', average_only=True)\n",
    "        logger.log_tabular('TotalEnvInteracts', t)\n",
    "        logger.log_tabular('QVals', with_min_and_max=True)\n",
    "        logger.log_tabular('LossQ', average_only=True)\n",
    "        logger.dump_tabular()\n",
    "\n",
    "env.close()\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file named config.json\n"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "data = get_datasets(dqn_output_dir)\n",
    "plot_data(data, xaxis='TotalEnvInteracts', value='Performance', smooth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I don't know why every time I run code above, the kernel will be dead. After I search many methods that maybe slove this, it still did not work and still was dead. Therefore, I can not get a figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run the trained agents as below (set render=True to visualize - note that additional settings may be required to view on Colab or on a remote server)\n",
    "\n",
    "Note: In order for this code to work, the DQN model definition should be in scope.\n",
    "\n",
    "The solved CartPole controller would typically have EpRet close to 200 (where 200 is the max) - If it doesn't quite plateau there, try rerunning the training (we're only training it for a short while with only sligtly tuned hyperparameters so it's possible that it may not stably plateau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading from dqn\\pyt_save\\model.pt.\n",
      "\n",
      "\n",
      "\u001b[32;1mLogging data to /tmp/experiments/1650421901\\progress.txt\u001b[0m\n",
      "Episode 0 \t EpRet 47.000 \t EpLen 47\n",
      "Episode 1 \t EpRet 200.000 \t EpLen 200\n",
      "Episode 2 \t EpRet 200.000 \t EpLen 200\n",
      "Episode 3 \t EpRet 200.000 \t EpLen 200\n",
      "Episode 4 \t EpRet 200.000 \t EpLen 200\n",
      "Episode 5 \t EpRet 200.000 \t EpLen 200\n",
      "Episode 6 \t EpRet 64.000 \t EpLen 64\n",
      "Episode 7 \t EpRet 200.000 \t EpLen 200\n",
      "Episode 8 \t EpRet 200.000 \t EpLen 200\n",
      "Episode 9 \t EpRet 200.000 \t EpLen 200\n",
      "-------------------------------------\n",
      "|    AverageEpRet |             171 |\n",
      "|        StdEpRet |            57.9 |\n",
      "|        MaxEpRet |             200 |\n",
      "|        MinEpRet |              47 |\n",
      "|           EpLen |             171 |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "playback_env, get_action = load_policy_and_env(dqn_output_dir, 'last', True)\n",
    "run_policy(playback_env, get_action, num_episodes=10, render=False)\n",
    "playback_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.2 Deep Deterministic Policy Gradient (DDPG) - An Actor-Critic Algorithm\n",
    "\n",
    "Next, we'll implement a Q-learning algorithm for continuous control using a popular actor-critic algortihm, DDPG.\n",
    "\n",
    "One of the main traits of actor-critic algortihsm is that unlike with DQNs:\n",
    "1. The policy network and value estimation network are separated into two separate, distinct functions\n",
    "2. They can be used to solve continuous controls problems, unlike DQNs which are limited to discrete action spaces.\n",
    "\n",
    "The Q-learning side of training the critic works similarly to DQNs, except that Q-networks now take states and actions both as inputs to estimate a Q-value, as opposed to simultaneously estimating Q values over all possible discrete actions.\n",
    "\n",
    "The Policy learning in DDPG is fairly simple. We want to learn a deterministic policy $\\pi_{\\theta}(s)$ (parameterized by $\\theta$) which gives the action that maximizes $Q_{\\phi}(s,a)$, a Q-value critic parameterized by $\\phi$. Because the action space is continuous, and we assume the Q-function is differentiable with respect to action, we can just perform gradient ascent (with respect to policy parameters only) to solve\n",
    "\n",
    "$\\max_{\\theta} \\mathbb{E}_{s \\sim {\\mathcal D}} [ Q_{\\phi}(s, \\pi_{\\theta}(s)) ]$\n",
    "\n",
    "Note that the Q-function parameters are treated as constants here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPActor(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, act_limit):\n",
    "        super().__init__()\n",
    "        pi_sizes = [obs_dim] + list(hidden_sizes) + [act_dim]\n",
    "        self.pi = mlp(pi_sizes, activation, nn.Tanh)\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Return output from network scaled to action space limits.\n",
    "        return self.act_limit * self.pi(obs)\n",
    "\n",
    "class MLPQFunction(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.q = mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        q = self.q(torch.cat([obs, act], dim=-1))\n",
    "        return torch.squeeze(q, -1) # Critical to ensure q has right shape.\n",
    "\n",
    "class MLPActorCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, observation_space, action_space, hidden_sizes=(256,256),\n",
    "                 activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "\n",
    "        obs_dim = observation_space.shape[0]\n",
    "        act_dim = action_space.shape[0]\n",
    "        act_limit = action_space.high[0]\n",
    "\n",
    "        # build policy and value functions\n",
    "        self.pi = MLPActor(obs_dim, act_dim, hidden_sizes, activation, act_limit)\n",
    "        self.q = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation)\n",
    "\n",
    "    def act(self, obs):\n",
    "        with torch.no_grad():\n",
    "            return self.pi(obs).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part of the homework, we'll test DDPG on a simple inverted pendulum problem.\n",
    "\n",
    "As before, in addition to the main trained networks, we build additional target networks to stabilize training.\n",
    "\n",
    "We will also set the logging directory to be a part of the current working directory but feel free to change that as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Log dir ddpg already exists! Storing info there anyway.\n",
      "\u001b[32;1mLogging data to ddpg\\progress.txt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Define the directory location to save DDPG logs and models\n",
    "ddpg_output_dir = 'ddpg'\n",
    "\n",
    "# Logger setup\n",
    "logger_kwargs={'output_dir':ddpg_output_dir, 'exp_name':'ddpg_pendulum'}\n",
    "logger = EpochLogger(**logger_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we'll be working on for DDPG is the (inverted) [Pendulum](https://gym.openai.com/envs/CartPole-v1/) OpenAI Gym task. \n",
    "\n",
    "The agent observes the angular position and velocity of the pendulum and can apply a continuous-valued torque to try and balance the pendulum.\n",
    "\n",
    "The episode keeps going for up to 200 steps and every step the agent gets reward mainly proportional to how far off it is from balancing the inverted pendulum, with smaller reward components related to how fast the pendulum is moving and how much torque is being applied. The objective is to maximize this reward, which is equivalent to balancing the inverted pendulum quickly and for as long as possible.\n",
    "\n",
    "Additional details for the implementation of this environment can be found via the code [source](https://github.com/openai/gym/blob/4ede9280f9c477f1ca09929d10cdc1e1ba1129f1/gym/envs/classic_control/pendulum.py).\n",
    "\n",
    "A random agent (or a poorly trained one) is unlikely to successfully balance the pendulum, as seen below:\n",
    "\n",
    "<img src=\"http://ai.bu.edu/DL523/HW5_files/pendulum_random_demo.gif\" width=\"360em\">\n",
    "\n",
    "But a well trained agent can successfully solve this problem (using solution code for this HW).\n",
    "\n",
    "<img src=\"http://ai.bu.edu/DL523/HW5_files/pendulum_demo.gif\" width=\"360em\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "steps_per_epoch=4000\n",
    "epochs=20\n",
    "gamma=0.99\n",
    "polyak=0.995\n",
    "batch_size=100\n",
    "start_steps=10000\n",
    "update_after=1000 \n",
    "update_every=50\n",
    "act_noise=0.1\n",
    "num_test_episodes=10\n",
    "max_ep_len=1000\n",
    "save_freq=1\n",
    "\n",
    "pi_lr=1e-3 \n",
    "q_lr=1e-3\n",
    "\n",
    "# Environment Definition\n",
    "env_fn = lambda :gym.make('Pendulum-v0')\n",
    "env, test_env = env_fn(), env_fn()\n",
    "obs_dim = env.observation_space.shape\n",
    "act_dim = env.action_space.shape[0]\n",
    "# Action limit for clamping: critically, assumes all dimensions share the same bound!\n",
    "act_limit = env.action_space.high[0]\n",
    "\n",
    "# Seeding\n",
    "seed=0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "test_env.seed(seed)\n",
    "\n",
    "# Experience buffer\n",
    "replay_size=int(1e6) \n",
    "replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)\n",
    "\n",
    "# Create actor-critic module and target networks\n",
    "ac = MLPActorCritic(env.observation_space, env.action_space)\n",
    "ac_targ = deepcopy(ac)\n",
    "\n",
    "# Freeze target networks with respect to optimizers (only update via polyak averaging)\n",
    "for p in ac_targ.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Set up model saving\n",
    "logger.setup_pytorch_saver(ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can next initialize modified helper functions - similar to the ones before but setup for the different architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def get_action(o, noise_scale):\n",
    "    a = ac.act(torch.as_tensor(o, dtype=torch.float32))\n",
    "    a += noise_scale * np.random.randn(act_dim)\n",
    "    return np.clip(a, -act_limit, act_limit)\n",
    "\n",
    "def test_agent():\n",
    "    for j in range(num_test_episodes):\n",
    "        o, d, ep_ret, ep_len = test_env.reset(), False, 0, 0\n",
    "        while not(d or (ep_len == max_ep_len)):\n",
    "            # Take deterministic actions at test time (noise_scale=0)\n",
    "            o, r, d, _ = test_env.step(get_action(o, 0))\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "        logger.store(TestEpRet=ep_ret, TestEpLen=ep_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define DDPG Q-learning Loss [10 points]\n",
    "\n",
    "This part is similar to DQN, except you'll need to modify your code to make use of the actor-critic separation appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up function for computing DDPG Q-loss\n",
    "def compute_loss_q(data):\n",
    "    o, a, r, o2, d = data['obs'], data['act'], data['rew'], data['obs2'], data['done']\n",
    "\n",
    "    \"\"\" STUDENT CODE GOES HERE \"\"\"\n",
    "    q = ac.q(o,a)\n",
    "        # Bellman backup for Q function\n",
    "    with torch.no_grad():\n",
    "        q_pi_targ = ac_targ.q(o2, ac_targ.pi(o2))\n",
    "        backup = r + gamma * (1 - d) * q_pi_targ\n",
    "\n",
    "        # MSE loss against Bellman backup\n",
    "    loss_q = ((q - backup)**2).mean()\n",
    "    \n",
    "    \"\"\" STUDENT CODE ENDS \"\"\"\n",
    "\n",
    "    # Useful info for logging\n",
    "    loss_info = dict(QVals=q.detach().numpy())\n",
    "\n",
    "    return loss_q, loss_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define DDPG Policy $\\pi$ Loss [10 points]\n",
    "\n",
    "The policy optimization loss is simple to compute. It can be computed as: $L_\\pi = -Q(o,\\pi(o))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up function for computing DDPG pi loss\n",
    "def compute_loss_pi(data):\n",
    "    o = data['obs']\n",
    "    \"\"\" STUDENT CODE GOES HERE \"\"\"\n",
    "    q_pi = ac.q(o, ac.pi(o))\n",
    "    pi_loss = -q_pi.mean()\n",
    "    \"\"\" STUDENT CODE ENDS \"\"\"\n",
    "    return pi_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall updates are handled similarly to DQN before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizers for policy and q-function\n",
    "pi_optimizer = Adam(ac.pi.parameters(), lr=pi_lr)\n",
    "q_optimizer = Adam(ac.q.parameters(), lr=q_lr)\n",
    "\n",
    "# Set up model saving\n",
    "logger.setup_pytorch_saver(ac)\n",
    "\n",
    "def update(data):\n",
    "    # First run one gradient descent step for Q.\n",
    "    q_optimizer.zero_grad()\n",
    "    loss_q, loss_info = compute_loss_q(data)\n",
    "    loss_q.backward()\n",
    "    q_optimizer.step()\n",
    "\n",
    "    # Freeze Q-network so you don't waste computational effort \n",
    "    # computing gradients for it during the policy learning step.\n",
    "    for p in ac.q.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Next run one gradient descent step for pi.\n",
    "    pi_optimizer.zero_grad()\n",
    "    loss_pi = compute_loss_pi(data)\n",
    "    loss_pi.backward()\n",
    "    pi_optimizer.step()\n",
    "\n",
    "    # Unfreeze Q-network so you can optimize it at next DDPG step.\n",
    "    for p in ac.q.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    # Record things\n",
    "    logger.store(LossQ=loss_q.item(), LossPi=loss_pi.item(), **loss_info)\n",
    "\n",
    "    # Finally, update target networks by polyak averaging.\n",
    "    with torch.no_grad():\n",
    "        for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):\n",
    "            # NB: We use an in-place operations \"mul_\", \"add_\" to update target\n",
    "            # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
    "            p_targ.data.mul_(polyak)\n",
    "            p_targ.data.add_((1 - polyak) * p.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run training.\n",
    "\n",
    "While running the code, pay attention the following logged information to determine if things are progressing well:\n",
    "\n",
    "1. For the first few thousand steps, the agents act randomly so you'll likely see low rewards that don't improve until more than `start_steps` steps have elapsed.\n",
    "2. Average Episode and Test Episode returns should generally be increasing from a very negative value to an average of > -150 for a successful agent.\n",
    "3. The policy loss (LossPi in the log as computed in `compute_loss_pi`) should decrease as the agent gets beter as this reflects how good the policy is at choosing 'good' actions.\n",
    "4. As before, we also expect the Q-value loss to decrease as the agent gets better at estimating Q-values.\n",
    "\n",
    "Unlike DQNs, where everything rides on a good estimation of Q-values, the policy behavior for actor-critic learning relies on critics learning useful Q-value estimations and actors learning useful policies based on the estimated Q-values. \n",
    "The separation offers increased flexibility and often improved representational power (in addition to being able to support continuous-valued actions) but does add extra knobs to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "|             Epoch |               1 |\n",
      "|      AverageEpRet |       -1.22e+03 |\n",
      "|          StdEpRet |             362 |\n",
      "|          MaxEpRet |            -758 |\n",
      "|          MinEpRet |       -1.79e+03 |\n",
      "|  AverageTestEpRet |            -512 |\n",
      "|      StdTestEpRet |             350 |\n",
      "|      MaxTestEpRet |          -0.766 |\n",
      "|      MinTestEpRet |            -851 |\n",
      "|             EpLen |             200 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |           4e+03 |\n",
      "|      AverageQVals |           -41.3 |\n",
      "|          StdQVals |            28.5 |\n",
      "|          MaxQVals |            1.24 |\n",
      "|          MinQVals |            -112 |\n",
      "|            LossPi |            40.2 |\n",
      "|             LossQ |            11.9 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |               2 |\n",
      "|      AverageEpRet |       -1.35e+03 |\n",
      "|          StdEpRet |             309 |\n",
      "|          MaxEpRet |            -756 |\n",
      "|          MinEpRet |       -1.77e+03 |\n",
      "|  AverageTestEpRet |            -103 |\n",
      "|      StdTestEpRet |            95.5 |\n",
      "|      MaxTestEpRet |            -1.1 |\n",
      "|      MinTestEpRet |            -273 |\n",
      "|             EpLen |             200 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |           8e+03 |\n",
      "|      AverageQVals |           -90.9 |\n",
      "|          StdQVals |            45.6 |\n",
      "|          MaxQVals |            23.1 |\n",
      "|          MinQVals |            -185 |\n",
      "|            LossPi |            87.6 |\n",
      "|             LossQ |            45.2 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |               3 |\n",
      "|      AverageEpRet |            -663 |\n",
      "|          StdEpRet |             553 |\n",
      "|          MaxEpRet |            -115 |\n",
      "|          MinEpRet |       -1.73e+03 |\n",
      "|  AverageTestEpRet |            -228 |\n",
      "|      StdTestEpRet |            98.2 |\n",
      "|      MaxTestEpRet |            -119 |\n",
      "|      MinTestEpRet |            -362 |\n",
      "|             EpLen |             200 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |         1.2e+04 |\n",
      "|      AverageQVals |            -102 |\n",
      "|          StdQVals |            69.1 |\n",
      "|          MaxQVals |            52.7 |\n",
      "|          MinQVals |            -212 |\n",
      "|            LossPi |            97.2 |\n",
      "|             LossQ |            68.9 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |               4 |\n",
      "|      AverageEpRet |            -121 |\n",
      "|          StdEpRet |            61.7 |\n",
      "|          MaxEpRet |           -3.07 |\n",
      "|          MinEpRet |            -231 |\n",
      "|  AverageTestEpRet |            -158 |\n",
      "|      StdTestEpRet |            91.2 |\n",
      "|      MaxTestEpRet |           -6.01 |\n",
      "|      MinTestEpRet |            -249 |\n",
      "|             EpLen |             200 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |         1.6e+04 |\n",
      "|      AverageQVals |           -58.9 |\n",
      "|          StdQVals |              90 |\n",
      "|          MaxQVals |            54.7 |\n",
      "|          MinQVals |            -222 |\n",
      "|            LossPi |            54.7 |\n",
      "|             LossQ |            51.1 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |               5 |\n",
      "|      AverageEpRet |            -205 |\n",
      "|          StdEpRet |             203 |\n",
      "|          MaxEpRet |           -1.53 |\n",
      "|          MinEpRet |            -958 |\n",
      "|  AverageTestEpRet |            -122 |\n",
      "|      StdTestEpRet |             108 |\n",
      "|      MaxTestEpRet |           -1.06 |\n",
      "|      MinTestEpRet |            -364 |\n",
      "|             EpLen |             200 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |           2e+04 |\n",
      "|      AverageQVals |           -35.3 |\n",
      "|          StdQVals |            81.4 |\n",
      "|          MaxQVals |              50 |\n",
      "|          MinQVals |            -218 |\n",
      "|            LossPi |            32.2 |\n",
      "|             LossQ |            34.4 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |               6 |\n",
      "|      AverageEpRet |            -181 |\n",
      "|          StdEpRet |            80.8 |\n",
      "|          MaxEpRet |            -117 |\n",
      "|          MinEpRet |            -376 |\n",
      "|  AverageTestEpRet |            -141 |\n",
      "|      StdTestEpRet |              43 |\n",
      "|      MaxTestEpRet |            -115 |\n",
      "|      MinTestEpRet |            -229 |\n",
      "|             EpLen |             200 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |         2.4e+04 |\n",
      "|      AverageQVals |           -27.9 |\n",
      "|          StdQVals |              75 |\n",
      "|          MaxQVals |              44 |\n",
      "|          MinQVals |            -206 |\n",
      "|            LossPi |            25.5 |\n",
      "|             LossQ |            28.2 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |               7 |\n",
      "|      AverageEpRet |            -176 |\n",
      "|          StdEpRet |            77.5 |\n",
      "|          MaxEpRet |           -4.98 |\n",
      "|          MinEpRet |            -312 |\n",
      "|  AverageTestEpRet |            -132 |\n",
      "|      StdTestEpRet |            61.5 |\n",
      "|      MaxTestEpRet |           -2.58 |\n",
      "|      MinTestEpRet |            -235 |\n",
      "|             EpLen |             200 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |         2.8e+04 |\n",
      "|      AverageQVals |           -25.4 |\n",
      "|          StdQVals |            71.7 |\n",
      "|          MaxQVals |            35.1 |\n",
      "|          MinQVals |            -211 |\n",
      "|            LossPi |            23.4 |\n",
      "|             LossQ |            25.7 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |               8 |\n",
      "|      AverageEpRet |            -153 |\n",
      "|          StdEpRet |             101 |\n",
      "|          MaxEpRet |          -0.626 |\n",
      "|          MinEpRet |            -358 |\n",
      "|  AverageTestEpRet |            -107 |\n",
      "|      StdTestEpRet |              61 |\n",
      "|      MaxTestEpRet |           -1.56 |\n",
      "|      MinTestEpRet |            -223 |\n",
      "|             EpLen |             200 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |         3.2e+04 |\n",
      "|      AverageQVals |           -24.1 |\n",
      "|          StdQVals |            70.6 |\n",
      "|          MaxQVals |            28.9 |\n",
      "|          MinQVals |            -219 |\n",
      "|            LossPi |            22.3 |\n",
      "|             LossQ |            25.4 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |               9 |\n",
      "|      AverageEpRet |            -188 |\n",
      "|          StdEpRet |             102 |\n",
      "|          MaxEpRet |            -4.1 |\n",
      "|          MinEpRet |            -370 |\n",
      "|  AverageTestEpRet |            -175 |\n",
      "|      StdTestEpRet |             113 |\n",
      "|      MaxTestEpRet |           -3.48 |\n",
      "|      MinTestEpRet |            -372 |\n",
      "|             EpLen |             200 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |         3.6e+04 |\n",
      "|      AverageQVals |           -23.2 |\n",
      "|          StdQVals |            69.8 |\n",
      "|          MaxQVals |            26.6 |\n",
      "|          MinQVals |            -226 |\n",
      "|            LossPi |            21.7 |\n",
      "|             LossQ |            23.1 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              10 |\n",
      "|      AverageEpRet |            -166 |\n",
      "|          StdEpRet |            91.2 |\n",
      "|          MaxEpRet |           -1.61 |\n",
      "|          MinEpRet |            -360 |\n",
      "|  AverageTestEpRet |            -145 |\n",
      "|      StdTestEpRet |            86.4 |\n",
      "|      MaxTestEpRet |           -2.41 |\n",
      "|      MinTestEpRet |            -347 |\n",
      "|             EpLen |             200 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |           4e+04 |\n",
      "|      AverageQVals |           -22.7 |\n",
      "|          StdQVals |            68.8 |\n",
      "|          MaxQVals |            24.7 |\n",
      "|          MinQVals |            -233 |\n",
      "|            LossPi |            21.3 |\n",
      "|             LossQ |            20.4 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              11 |\n",
      "|      AverageEpRet |            -131 |\n",
      "|          StdEpRet |              96 |\n",
      "|          MaxEpRet |          -0.559 |\n",
      "|          MinEpRet |            -348 |\n",
      "|  AverageTestEpRet |            -200 |\n",
      "|      StdTestEpRet |            89.4 |\n",
      "|      MaxTestEpRet |            -116 |\n",
      "|      MinTestEpRet |            -348 |\n",
      "|             EpLen |             200 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |         4.4e+04 |\n",
      "|      AverageQVals |           -22.9 |\n",
      "|          StdQVals |            67.1 |\n",
      "|          MaxQVals |            21.7 |\n",
      "|          MinQVals |            -236 |\n",
      "|            LossPi |            21.6 |\n",
      "|             LossQ |            21.8 |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "|             Epoch |              12 |\n",
      "|      AverageEpRet |            -163 |\n",
      "|          StdEpRet |             106 |\n",
      "|          MaxEpRet |           -2.16 |\n",
      "|          MinEpRet |            -368 |\n",
      "|  AverageTestEpRet |            -157 |\n",
      "|      StdTestEpRet |            49.7 |\n",
      "|      MaxTestEpRet |            -117 |\n",
      "|      MinTestEpRet |            -242 |\n",
      "|             EpLen |             200 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |         4.8e+04 |\n",
      "|      AverageQVals |           -23.1 |\n",
      "|          StdQVals |            65.6 |\n",
      "|          MaxQVals |            18.8 |\n",
      "|          MinQVals |            -241 |\n",
      "|            LossPi |              22 |\n",
      "|             LossQ |            19.4 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              13 |\n",
      "|      AverageEpRet |            -192 |\n",
      "|          StdEpRet |            96.8 |\n",
      "|          MaxEpRet |           -3.84 |\n",
      "|          MinEpRet |            -374 |\n",
      "|  AverageTestEpRet |            -170 |\n",
      "|      StdTestEpRet |            56.2 |\n",
      "|      MaxTestEpRet |            -120 |\n",
      "|      MinTestEpRet |            -244 |\n",
      "|             EpLen |             200 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |         5.2e+04 |\n",
      "|      AverageQVals |           -23.9 |\n",
      "|          StdQVals |            63.8 |\n",
      "|          MaxQVals |            14.7 |\n",
      "|          MinQVals |            -243 |\n",
      "|            LossPi |            22.9 |\n",
      "|             LossQ |            16.7 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              14 |\n",
      "|      AverageEpRet |            -145 |\n",
      "|          StdEpRet |            85.1 |\n",
      "|          MaxEpRet |           -2.39 |\n",
      "|          MinEpRet |            -357 |\n",
      "|  AverageTestEpRet |            -186 |\n",
      "|      StdTestEpRet |              94 |\n",
      "|      MaxTestEpRet |           -11.2 |\n",
      "|      MinTestEpRet |            -362 |\n",
      "|             EpLen |             200 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |         5.6e+04 |\n",
      "|      AverageQVals |           -24.3 |\n",
      "|          StdQVals |            62.8 |\n",
      "|          MaxQVals |            12.9 |\n",
      "|          MinQVals |            -247 |\n",
      "|            LossPi |            23.3 |\n",
      "|             LossQ |            15.5 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              15 |\n",
      "|      AverageEpRet |            -133 |\n",
      "|          StdEpRet |            76.7 |\n",
      "|          MaxEpRet |           -6.93 |\n",
      "|          MinEpRet |            -357 |\n",
      "|  AverageTestEpRet |            -135 |\n",
      "|      StdTestEpRet |              81 |\n",
      "|      MaxTestEpRet |           -5.61 |\n",
      "|      MinTestEpRet |            -253 |\n",
      "|             EpLen |             200 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |           6e+04 |\n",
      "|      AverageQVals |             -22 |\n",
      "|          StdQVals |            62.9 |\n",
      "|          MaxQVals |            13.4 |\n",
      "|          MinQVals |            -252 |\n",
      "|            LossPi |            20.9 |\n",
      "|             LossQ |              15 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              16 |\n",
      "|      AverageEpRet |            -146 |\n",
      "|          StdEpRet |            80.1 |\n",
      "|          MaxEpRet |           -9.53 |\n",
      "|          MinEpRet |            -333 |\n",
      "|  AverageTestEpRet |            -159 |\n",
      "|      StdTestEpRet |            87.4 |\n",
      "|      MaxTestEpRet |           -8.51 |\n",
      "|      MinTestEpRet |            -345 |\n",
      "|             EpLen |             200 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |         6.4e+04 |\n",
      "|      AverageQVals |           -19.3 |\n",
      "|          StdQVals |            61.5 |\n",
      "|          MaxQVals |            14.8 |\n",
      "|          MinQVals |            -251 |\n",
      "|            LossPi |            18.4 |\n",
      "|             LossQ |            16.6 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              17 |\n",
      "|      AverageEpRet |            -142 |\n",
      "|          StdEpRet |            75.9 |\n",
      "|          MaxEpRet |            -5.3 |\n",
      "|          MinEpRet |            -350 |\n",
      "|  AverageTestEpRet |            -177 |\n",
      "|      StdTestEpRet |            86.9 |\n",
      "|      MaxTestEpRet |            -122 |\n",
      "|      MinTestEpRet |            -342 |\n",
      "|             EpLen |             200 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |         6.8e+04 |\n",
      "|      AverageQVals |           -18.5 |\n",
      "|          StdQVals |              60 |\n",
      "|          MaxQVals |            14.3 |\n",
      "|          MinQVals |            -247 |\n",
      "|            LossPi |            17.7 |\n",
      "|             LossQ |              16 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              18 |\n",
      "|      AverageEpRet |            -171 |\n",
      "|          StdEpRet |              94 |\n",
      "|          MaxEpRet |           -2.66 |\n",
      "|          MinEpRet |            -339 |\n",
      "|  AverageTestEpRet |            -201 |\n",
      "|      StdTestEpRet |             113 |\n",
      "|      MaxTestEpRet |           -5.73 |\n",
      "|      MinTestEpRet |            -369 |\n",
      "|             EpLen |             200 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |         7.2e+04 |\n",
      "|      AverageQVals |           -18.2 |\n",
      "|          StdQVals |            58.7 |\n",
      "|          MaxQVals |            13.5 |\n",
      "|          MinQVals |            -253 |\n",
      "|            LossPi |            17.4 |\n",
      "|             LossQ |            13.6 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              19 |\n",
      "|      AverageEpRet |            -160 |\n",
      "|          StdEpRet |            87.4 |\n",
      "|          MaxEpRet |           -3.38 |\n",
      "|          MinEpRet |            -321 |\n",
      "|  AverageTestEpRet |            -144 |\n",
      "|      StdTestEpRet |             101 |\n",
      "|      MaxTestEpRet |           -2.25 |\n",
      "|      MinTestEpRet |            -349 |\n",
      "|             EpLen |             200 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |         7.6e+04 |\n",
      "|      AverageQVals |           -17.8 |\n",
      "|          StdQVals |            57.7 |\n",
      "|          MaxQVals |            13.6 |\n",
      "|          MinQVals |            -251 |\n",
      "|            LossPi |              17 |\n",
      "|             LossQ |            12.6 |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "|             Epoch |              20 |\n",
      "|      AverageEpRet |            -161 |\n",
      "|          StdEpRet |            72.8 |\n",
      "|          MaxEpRet |           -4.16 |\n",
      "|          MinEpRet |            -338 |\n",
      "|  AverageTestEpRet |            -184 |\n",
      "|      StdTestEpRet |            65.8 |\n",
      "|      MaxTestEpRet |            -116 |\n",
      "|      MinTestEpRet |            -307 |\n",
      "|             EpLen |             200 |\n",
      "|         TestEpLen |             200 |\n",
      "| TotalEnvInteracts |           8e+04 |\n",
      "|      AverageQVals |           -17.5 |\n",
      "|          StdQVals |            56.7 |\n",
      "|          MaxQVals |            15.1 |\n",
      "|          MinQVals |            -249 |\n",
      "|            LossPi |            16.8 |\n",
      "|             LossQ |            12.8 |\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Prepare for interaction with environment\n",
    "total_steps = steps_per_epoch * epochs\n",
    "o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "# Main loop: collect experience in env and update/log each epoch\n",
    "for t in range(total_steps):\n",
    "    \n",
    "    # Until start_steps have elapsed, randomly sample actions\n",
    "    # from a uniform distribution for better exploration. Afterwards, \n",
    "    # use the learned policy (with some noise, via act_noise). \n",
    "    if t > start_steps:\n",
    "        a = get_action(o, act_noise)\n",
    "    else:\n",
    "        a = env.action_space.sample()\n",
    "\n",
    "    # Step the env\n",
    "    o2, r, d, _ = env.step(a)\n",
    "    ep_ret += r\n",
    "    ep_len += 1\n",
    "\n",
    "    # Ignore the \"done\" signal if it comes from hitting the time\n",
    "    # horizon (that is, when it's an artificial terminal signal\n",
    "    # that isn't based on the agent's state)\n",
    "    d = False if ep_len==max_ep_len else d\n",
    "\n",
    "    # Store experience to replay buffer\n",
    "    replay_buffer.store(o, a, r, o2, d)\n",
    "\n",
    "    # Super critical, easy to overlook step: make sure to update \n",
    "    # most recent observation!\n",
    "    o = o2\n",
    "\n",
    "    # End of trajectory handling\n",
    "    if d or (ep_len == max_ep_len):\n",
    "        logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
    "        o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "    # Update handling\n",
    "    if t >= update_after and t % update_every == 0:\n",
    "        for _ in range(update_every):\n",
    "            batch = replay_buffer.sample_batch(batch_size)\n",
    "            update(data=batch)\n",
    "\n",
    "    # End of epoch handling\n",
    "    if (t+1) % steps_per_epoch == 0:\n",
    "        epoch = (t+1) // steps_per_epoch\n",
    "\n",
    "        # Save model\n",
    "        if (epoch % save_freq == 0) or (epoch == epochs):\n",
    "            logger.save_state({'env': env}, None)\n",
    "\n",
    "        # Test the performance of the deterministic version of the agent.\n",
    "        test_agent()\n",
    "\n",
    "        # Log info about epoch\n",
    "        logger.log_tabular('Epoch', epoch)\n",
    "        logger.log_tabular('EpRet', with_min_and_max=True)\n",
    "        logger.log_tabular('TestEpRet', with_min_and_max=True)\n",
    "        logger.log_tabular('EpLen', average_only=True)\n",
    "        logger.log_tabular('TestEpLen', average_only=True)\n",
    "        logger.log_tabular('TotalEnvInteracts', t)\n",
    "        logger.log_tabular('QVals', with_min_and_max=True)\n",
    "        logger.log_tabular('LossPi', average_only=True)\n",
    "        logger.log_tabular('LossQ', average_only=True)\n",
    "        logger.dump_tabular()\n",
    "\n",
    "env.close()\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file named config.json\n"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "data = get_datasets(ddpg_output_dir)\n",
    "plot_data(data, xaxis='TotalEnvInteracts', value='Performance', smooth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I don't know why every time I run code above, the kernel will be dead. After I search many methods that maybe slove this, it still did not work and still was dead. Therefore, I can not get a figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run the trained agents as below (set render=True to visualize - note that additional settings may be required to view on Colab or on a remote server)\n",
    "\n",
    "Note: In order for this code to work, the MLPActorCritic model definition (and dependencies) should be in scope.\n",
    "\n",
    "A solved pendulum controller would typically have EpRet > -150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading from ddpg\\pyt_save\\model.pt.\n",
      "\n",
      "\n",
      "\u001b[32;1mLogging data to /tmp/experiments/1650422483\\progress.txt\u001b[0m\n",
      "Episode 0 \t EpRet -127.627 \t EpLen 200\n",
      "Episode 1 \t EpRet -122.088 \t EpLen 200\n",
      "Episode 2 \t EpRet -2.412 \t EpLen 200\n",
      "Episode 3 \t EpRet -224.994 \t EpLen 200\n",
      "Episode 4 \t EpRet -120.048 \t EpLen 200\n",
      "Episode 5 \t EpRet -226.267 \t EpLen 200\n",
      "Episode 6 \t EpRet -2.695 \t EpLen 200\n",
      "Episode 7 \t EpRet -116.887 \t EpLen 200\n",
      "Episode 8 \t EpRet -117.410 \t EpLen 200\n",
      "Episode 9 \t EpRet -122.378 \t EpLen 200\n",
      "-------------------------------------\n",
      "|    AverageEpRet |            -118 |\n",
      "|        StdEpRet |            70.7 |\n",
      "|        MaxEpRet |           -2.41 |\n",
      "|        MinEpRet |            -226 |\n",
      "|           EpLen |             200 |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "playback_env, get_action = load_policy_and_env(ddpg_output_dir, 'last', True)\n",
    "run_policy(playback_env, get_action, num_episodes=10, render=False)\n",
    "playback_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tMPpAqRpR2N"
   },
   "source": [
    "## Q2 Sequence to Sequence Modelling with nn.Transformer and Torch Text (20 points)\n",
    "\n",
    "You will implement a part of transformer. This question aims to let you to get familiar with the transformer architecture purposed in the paper [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf). This question is modified from the original pytorch tutorial [here](https://pytorch.org/tutorials/beginner/transformer_tutorial.html?highlight=transformer), you can refer it when you fill out the code. The general architecture of trasnsformer is shown in the figure below:\n",
    "\n",
    "<img src=\"http://ai.bu.edu/DL523/HW5_files/transformer_architecture.jpg\" width=\"360em\">\n",
    "\n",
    "This question requires you to implement a sequence to sequence model by encoder, which is the left part of the figure. You will use integrated layers in pytorch.\n",
    "\n",
    "The transformer model has been proved to be superior in quality for many sequence-to-sequence\n",
    "problems while being more parallelizable. The ``nn.Transformer`` module\n",
    "relies entirely on an attention mechanism (another module recently\n",
    "implemented as `nn.MultiheadAttention`) to draw global dependencies\n",
    "between input and output. The ``nn.Transformer`` module is now highly\n",
    "modularized such that a single component (like [`nn.TransformerEncoder `](<https://pytorch.org/docs/master/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder>)\n",
    "in this tutorial) can be easily adapted/composed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkeaGn8INY9k"
   },
   "source": [
    "### Q2.1 Define the model \n",
    "In this question, we train ``nn.TransformerEncoder`` model on a\n",
    "language modeling task. The language modeling task is to assign a\n",
    "probability for the likelihood of a given word (or a sequence of words)\n",
    "to follow a sequence of words. A sequence of tokens are passed to the embedding\n",
    "layer first, followed by a positional encoding layer to account for the order\n",
    "of the word (see the next paragraph for more details). The\n",
    "``nn.TransformerEncoder`` consists of multiple layers of\n",
    "``nn.TransformerEncoderLayer`` . Along with the input sequence, a square\n",
    "attention mask is required because the self-attention layers in\n",
    "``nn.TransformerEncoder`` are only allowed to attend the earlier positions in\n",
    "the sequence. For the language modeling task, any tokens on the future\n",
    "positions should be masked. To have the actual words, the output\n",
    "of ``nn.TransformerEncoder`` model is sent to the final Linear\n",
    "layer, which is followed by a log-Softmax function. We will see how to implement the ``PositionalEncoding`` in the later question. \n",
    "\n",
    "<img src=\"http://ai.bu.edu/DL523/HW5_files/encoder.png\" width=\"em\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHI3LBIcgGVO"
   },
   "source": [
    "In the following model, we only train a encoder model, which is the left part of the figure. Then we concatenate a Linear model `self.decoder` to replace the right part of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO AVOID ISSUES WITH PACKAGE IMPORTS, IT MIGHT BE WORTH RESTARTING THE NOTEBOOK KERNEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ai9dTxjUNS5-"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    '''\n",
    "    This is a transformer encoder model, the input arguments are as follows:\n",
    "    args:\n",
    "    ntoken:  dimension of tokens\n",
    "    ninp: dimension of input embeddings\n",
    "    nhid: dimension of the hidden encoding between two layers of TransformerEncoderLayer\n",
    "    nlayers: number of TransformerEncoderLayer layers\n",
    "    nhead: the number of heads in the multiheadattention model\n",
    "    '''\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout) # PositionalEncoding will be implemented in next section.\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        \"\"\"YOUR CODE HERE\"\"\"\n",
    "        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"YOUR CODE HERE\"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-BxUnw6MZf5"
   },
   "source": [
    "### Q2.2 Positional Encoding\n",
    "#### Q2.2.1 Fill the code block\n",
    "``PositionalEncoding`` module injects some information about the\n",
    "relative or absolute position of the tokens in the sequence. The\n",
    "positional encodings have the same dimension as the embeddings so that\n",
    "the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
    "different frequencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6A0pUKNMpQ84"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \"\"\"YOUR CODE HERE\"\"\"\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \"\"\"YOUR CODE ENDS\"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"YOUR CODE HERE\"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        output = self.dropout(x)\n",
    "        \"\"\"YOUR CODE ENDS\"\"\"\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7mD6P7eCaKs"
   },
   "source": [
    "#### Q2.2.2 Why do we need this positional encoding in the transformer architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`[double click here to add a solution]`**\n",
    "\n",
    "Compared with the traditional RNN, Self-Attention has no input sequence in input calculation, but adopts the idea of parallelization to speed up the operation, so that Self-Attention can process the next token at the same time when the result of the previous token has not come out. However, while parallelization improves the operation speed, it also brings a problem, that is, the sequentiality of the sequence is lost. Therefore, in order not to lose order, it is necessary to combine positional encoding before inputting the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i82IJ2chNstR"
   },
   "source": [
    "### Q2.3 Running the model\n",
    "\n",
    "#### Q2.3.1 Run the code to get desired performance.\n",
    "The training process uses Wikitext-2 dataset from ``torchtext``. The\n",
    "vocab object is built based on the train dataset and is used to numericalize\n",
    "tokens into tensors. Starting from sequential data, the ``batchify()``\n",
    "function arranges the dataset into columns, trimming off any tokens remaining\n",
    "after the data has been divided into batches of size ``batch_size``.\n",
    "For instance, with the alphabet as the sequence (total length of 26)\n",
    "and a batch size of 4, we would divide the alphabet into 4 sequences of\n",
    "length 6:\n",
    "\n",
    "$$\n",
    "\\begin{align}\\begin{bmatrix}\n",
    "  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
    "  \\end{bmatrix}\n",
    "  \\Rightarrow\n",
    "  \\begin{bmatrix}\n",
    "  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
    "  \\end{bmatrix}\\end{align}\n",
    "$$\n",
    "\n",
    "These columns are treated as independent by the model, which means that\n",
    "the dependence of ``G`` and ``F`` can not be learned, but allows more\n",
    "efficient batch processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**, we're using torchtext v 0.11.0\n",
    "\n",
    "You *may* need to run the following code block below if running locally if errors are thrown about missing packages or components (though check the rest first before going through this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext==0.11 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (0.11.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -lotly (d:\\anaconda3\\envs\\torch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (d:\\anaconda3\\envs\\torch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (d:\\anaconda3\\envs\\torch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (d:\\anaconda3\\envs\\torch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (d:\\anaconda3\\envs\\torch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (d:\\anaconda3\\envs\\torch\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: torch==1.10.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from torchtext==0.11) (1.10.0)\n",
      "Requirement already satisfied: requests in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from torchtext==0.11) (2.27.1)\n",
      "Requirement already satisfied: numpy in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from torchtext==0.11) (1.21.1)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from torchtext==0.11) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from torch==1.10.0->torchtext==0.11) (3.10.0.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->torchtext==0.11) (2.0.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->torchtext==0.11) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->torchtext==0.11) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->torchtext==0.11) (1.26.8)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from tqdm->torchtext==0.11) (0.4.4)\n",
      "Requirement already satisfied: spacy<=3.2.4,>=2.2.4 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (3.2.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<=3.2.4,>=2.2.4) (2.4.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<=3.2.4,>=2.2.4) (1.8.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<=3.2.4,>=2.2.4) (1.0.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<=3.2.4,>=2.2.4) (0.7.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<=3.2.4,>=2.2.4) (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<=3.2.4,>=2.2.4) (2.27.1)\n",
      "Requirement already satisfied: click<8.1.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<=3.2.4,>=2.2.4) (8.0.4)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<=3.2.4,>=2.2.4) (49.6.0.post20210108)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<=3.2.4,>=2.2.4) (2.0.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<=3.2.4,>=2.2.4) (1.21.1)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<=3.2.4,>=2.2.4) (3.0.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<=3.2.4,>=2.2.4) (0.6.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<=3.2.4,>=2.2.4) (0.4.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<=3.2.4,>=2.2.4) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<=3.2.4,>=2.2.4) (3.10.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<=3.2.4,>=2.2.4) (21.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<=3.2.4,>=2.2.4) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<=3.2.4,>=2.2.4) (0.9.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<=3.2.4,>=2.2.4) (1.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<=3.2.4,>=2.2.4) (4.62.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<=3.2.4,>=2.2.4) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<=3.2.4,>=2.2.4) (8.0.15)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from catalogue<2.1.0,>=2.0.6->spacy<=3.2.4,>=2.2.4) (3.7.0)\n",
      "Requirement already satisfied: importlib-metadata in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from click<8.1.0->spacy<=3.2.4,>=2.2.4) (4.10.1)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from click<8.1.0->spacy<=3.2.4,>=2.2.4) (0.4.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from packaging>=20.0->spacy<=3.2.4,>=2.2.4) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from pathy>=0.3.5->spacy<=3.2.4,>=2.2.4) (5.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<=3.2.4,>=2.2.4) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<=3.2.4,>=2.2.4) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<=3.2.4,>=2.2.4) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<=3.2.4,>=2.2.4) (2.0.11)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from jinja2->spacy<=3.2.4,>=2.2.4) (2.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -lotly (d:\\anaconda3\\envs\\torch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (d:\\anaconda3\\envs\\torch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (d:\\anaconda3\\envs\\torch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (d:\\anaconda3\\envs\\torch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (d:\\anaconda3\\envs\\torch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (d:\\anaconda3\\envs\\torch\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from en-core-web-sm==3.2.0) (3.2.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.15)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.27.1)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.1)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.7)\n",
      "Requirement already satisfied: click<8.1.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.0)\n",
      "Requirement already satisfied: importlib-metadata in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from click<8.1.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.10.1)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from click<8.1.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda3\\envs\\torch\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -lotly (d:\\anaconda3\\envs\\torch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (d:\\anaconda3\\envs\\torch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (d:\\anaconda3\\envs\\torch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (d:\\anaconda3\\envs\\torch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (d:\\anaconda3\\envs\\torch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (d:\\anaconda3\\envs\\torch\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"torchtext==0.11\"\n",
    "!pip install \"spacy>=2.2.4,<=3.2.4\"\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kv4yr-K3OHSs",
    "outputId": "06f95231-18cc-46ba-fd7e-ddf60ef3c774"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\torch\\lib\\site-packages\\torchtext\\data\\utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading wikitext-2-v1.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.48M/4.48M [00:00<00:00, 5.22MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "from torchtext.legacy import data\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "TEXT = torchtext.legacy.data.Field(tokenize=get_tokenizer(\"spacy\"),\n",
    "                            init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)\n",
    "train_txt, val_txt, test_txt = torchtext.legacy.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(train_txt)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    data = TEXT.numericalize([data.examples[0].text])\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_txt, batch_size)\n",
    "val_data = batchify(val_txt, eval_batch_size)\n",
    "test_data = batchify(test_txt, eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjC_fNXWOcIJ"
   },
   "source": [
    "The ``get_batch()`` function generates the input and target sequence for\n",
    "the transformer model. It subdivides the source data into chunks of\n",
    "length ``bptt``. For the language modeling task, the model needs the\n",
    "following words as ``Target``. For example, with a ``bptt`` value of 2,\n",
    "weâ€™d get the following two Variables for ``i`` = 0:\n",
    "\n",
    "<img src=\"http://ai.bu.edu/DL523/HW5_files/transformer_input_target1.png\" width=\"em\">\n",
    "<!-- ![](transformer_input_target.png) -->\n",
    "\n",
    "\n",
    "It should be noted that the chunks are along dimension 0, consistent\n",
    "with the ``S`` dimension in the Transformer model. The batch dimension\n",
    "``N`` is along dimension 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hxe4vOD8Oh7S"
   },
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n",
    "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6Ss9NoUZY0Rz"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "import time\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    print(\"in training loop\")\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        if data.size(0) != bptt:\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 10\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            if data.size(0) != bptt:\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            output = eval_model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5niRh2AI6qeN"
   },
   "source": [
    "Running the code block below. You will get around 220 ppl on training at the end of epoch 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1CB2g1K5ZDBf",
    "outputId": "1ceef4af-b113-41d9-8e0f-f32bc21302b2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "in training loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:370: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    10/ 3195 batches | lr 5.00 | ms/batch 278.57 | loss 11.07 | ppl 63897.18\n",
      "| epoch   1 |    20/ 3195 batches | lr 5.00 | ms/batch 254.37 | loss  9.19 | ppl  9844.82\n",
      "| epoch   1 |    30/ 3195 batches | lr 5.00 | ms/batch 248.68 | loss  8.53 | ppl  5060.00\n",
      "| epoch   1 |    40/ 3195 batches | lr 5.00 | ms/batch 252.17 | loss  7.81 | ppl  2458.86\n",
      "| epoch   1 |    50/ 3195 batches | lr 5.00 | ms/batch 249.91 | loss  7.50 | ppl  1809.05\n",
      "| epoch   1 |    60/ 3195 batches | lr 5.00 | ms/batch 252.27 | loss  7.55 | ppl  1901.50\n",
      "| epoch   1 |    70/ 3195 batches | lr 5.00 | ms/batch 252.85 | loss  7.10 | ppl  1215.47\n",
      "| epoch   1 |    80/ 3195 batches | lr 5.00 | ms/batch 243.09 | loss  7.13 | ppl  1251.58\n",
      "| epoch   1 |    90/ 3195 batches | lr 5.00 | ms/batch 251.10 | loss  6.84 | ppl   932.06\n",
      "| epoch   1 |   100/ 3195 batches | lr 5.00 | ms/batch 248.07 | loss  7.04 | ppl  1146.63\n",
      "| epoch   1 |   110/ 3195 batches | lr 5.00 | ms/batch 252.48 | loss  6.91 | ppl   998.88\n",
      "| epoch   1 |   120/ 3195 batches | lr 5.00 | ms/batch 260.44 | loss  6.78 | ppl   877.42\n",
      "| epoch   1 |   130/ 3195 batches | lr 5.00 | ms/batch 241.96 | loss  6.80 | ppl   902.01\n",
      "| epoch   1 |   140/ 3195 batches | lr 5.00 | ms/batch 240.79 | loss  6.81 | ppl   902.69\n",
      "| epoch   1 |   150/ 3195 batches | lr 5.00 | ms/batch 237.71 | loss  6.79 | ppl   886.53\n",
      "| epoch   1 |   160/ 3195 batches | lr 5.00 | ms/batch 239.08 | loss  6.66 | ppl   780.77\n",
      "| epoch   1 |   170/ 3195 batches | lr 5.00 | ms/batch 258.07 | loss  6.49 | ppl   661.21\n",
      "| epoch   1 |   180/ 3195 batches | lr 5.00 | ms/batch 248.48 | loss  6.58 | ppl   720.00\n",
      "| epoch   1 |   190/ 3195 batches | lr 5.00 | ms/batch 247.09 | loss  6.60 | ppl   737.43\n",
      "| epoch   1 |   200/ 3195 batches | lr 5.00 | ms/batch 245.49 | loss  6.62 | ppl   749.98\n",
      "| epoch   1 |   210/ 3195 batches | lr 5.00 | ms/batch 245.48 | loss  6.74 | ppl   848.69\n",
      "| epoch   1 |   220/ 3195 batches | lr 5.00 | ms/batch 241.69 | loss  6.61 | ppl   745.68\n",
      "| epoch   1 |   230/ 3195 batches | lr 5.00 | ms/batch 238.26 | loss  6.25 | ppl   520.03\n",
      "| epoch   1 |   240/ 3195 batches | lr 5.00 | ms/batch 242.30 | loss  6.40 | ppl   599.09\n",
      "| epoch   1 |   250/ 3195 batches | lr 5.00 | ms/batch 236.35 | loss  6.24 | ppl   513.49\n",
      "| epoch   1 |   260/ 3195 batches | lr 5.00 | ms/batch 237.32 | loss  6.40 | ppl   602.55\n",
      "| epoch   1 |   270/ 3195 batches | lr 5.00 | ms/batch 243.90 | loss  6.38 | ppl   587.73\n",
      "| epoch   1 |   280/ 3195 batches | lr 5.00 | ms/batch 239.19 | loss  6.21 | ppl   498.79\n",
      "| epoch   1 |   290/ 3195 batches | lr 5.00 | ms/batch 232.98 | loss  6.33 | ppl   561.94\n",
      "| epoch   1 |   300/ 3195 batches | lr 5.00 | ms/batch 236.06 | loss  6.44 | ppl   626.58\n",
      "| epoch   1 |   310/ 3195 batches | lr 5.00 | ms/batch 242.71 | loss  6.32 | ppl   555.03\n",
      "| epoch   1 |   320/ 3195 batches | lr 5.00 | ms/batch 236.58 | loss  6.17 | ppl   479.69\n",
      "| epoch   1 |   330/ 3195 batches | lr 5.00 | ms/batch 238.02 | loss  6.25 | ppl   520.07\n",
      "| epoch   1 |   340/ 3195 batches | lr 5.00 | ms/batch 239.62 | loss  6.09 | ppl   441.50\n",
      "| epoch   1 |   350/ 3195 batches | lr 5.00 | ms/batch 240.71 | loss  6.01 | ppl   408.51\n",
      "| epoch   1 |   360/ 3195 batches | lr 5.00 | ms/batch 241.41 | loss  6.09 | ppl   441.02\n",
      "| epoch   1 |   370/ 3195 batches | lr 5.00 | ms/batch 247.54 | loss  6.04 | ppl   420.85\n",
      "| epoch   1 |   380/ 3195 batches | lr 5.00 | ms/batch 245.26 | loss  6.17 | ppl   479.57\n",
      "| epoch   1 |   390/ 3195 batches | lr 5.00 | ms/batch 242.12 | loss  6.09 | ppl   442.80\n",
      "| epoch   1 |   400/ 3195 batches | lr 5.00 | ms/batch 242.86 | loss  6.10 | ppl   445.23\n",
      "| epoch   1 |   410/ 3195 batches | lr 5.00 | ms/batch 240.11 | loss  6.32 | ppl   555.75\n",
      "| epoch   1 |   420/ 3195 batches | lr 5.00 | ms/batch 238.29 | loss  6.04 | ppl   418.97\n",
      "| epoch   1 |   430/ 3195 batches | lr 5.00 | ms/batch 241.14 | loss  5.99 | ppl   398.40\n",
      "| epoch   1 |   440/ 3195 batches | lr 5.00 | ms/batch 237.03 | loss  6.13 | ppl   460.41\n",
      "| epoch   1 |   450/ 3195 batches | lr 5.00 | ms/batch 241.08 | loss  5.96 | ppl   387.99\n",
      "| epoch   1 |   460/ 3195 batches | lr 5.00 | ms/batch 236.17 | loss  5.82 | ppl   335.41\n",
      "| epoch   1 |   470/ 3195 batches | lr 5.00 | ms/batch 238.58 | loss  6.05 | ppl   422.11\n",
      "| epoch   1 |   480/ 3195 batches | lr 5.00 | ms/batch 240.44 | loss  5.88 | ppl   357.11\n",
      "| epoch   1 |   490/ 3195 batches | lr 5.00 | ms/batch 241.05 | loss  6.15 | ppl   468.23\n",
      "| epoch   1 |   500/ 3195 batches | lr 5.00 | ms/batch 245.48 | loss  5.83 | ppl   341.09\n",
      "| epoch   1 |   510/ 3195 batches | lr 5.00 | ms/batch 248.79 | loss  5.98 | ppl   396.02\n",
      "| epoch   1 |   520/ 3195 batches | lr 5.00 | ms/batch 253.19 | loss  6.06 | ppl   428.44\n",
      "| epoch   1 |   530/ 3195 batches | lr 5.00 | ms/batch 258.85 | loss  6.01 | ppl   408.93\n",
      "| epoch   1 |   540/ 3195 batches | lr 5.00 | ms/batch 250.02 | loss  5.87 | ppl   354.72\n",
      "| epoch   1 |   550/ 3195 batches | lr 5.00 | ms/batch 243.15 | loss  5.86 | ppl   351.05\n",
      "| epoch   1 |   560/ 3195 batches | lr 5.00 | ms/batch 242.72 | loss  5.94 | ppl   380.27\n",
      "| epoch   1 |   570/ 3195 batches | lr 5.00 | ms/batch 241.65 | loss  5.91 | ppl   368.48\n",
      "| epoch   1 |   580/ 3195 batches | lr 5.00 | ms/batch 245.54 | loss  5.90 | ppl   363.96\n",
      "| epoch   1 |   590/ 3195 batches | lr 5.00 | ms/batch 240.79 | loss  5.84 | ppl   342.38\n",
      "| epoch   1 |   600/ 3195 batches | lr 5.00 | ms/batch 243.01 | loss  5.82 | ppl   337.51\n",
      "| epoch   1 |   610/ 3195 batches | lr 5.00 | ms/batch 243.56 | loss  5.84 | ppl   344.46\n",
      "| epoch   1 |   620/ 3195 batches | lr 5.00 | ms/batch 240.67 | loss  5.87 | ppl   354.47\n",
      "| epoch   1 |   630/ 3195 batches | lr 5.00 | ms/batch 239.00 | loss  5.76 | ppl   316.51\n",
      "| epoch   1 |   640/ 3195 batches | lr 5.00 | ms/batch 248.12 | loss  5.70 | ppl   298.67\n",
      "| epoch   1 |   650/ 3195 batches | lr 5.00 | ms/batch 242.52 | loss  6.04 | ppl   419.60\n",
      "| epoch   1 |   660/ 3195 batches | lr 5.00 | ms/batch 236.33 | loss  5.85 | ppl   347.33\n",
      "| epoch   1 |   670/ 3195 batches | lr 5.00 | ms/batch 237.77 | loss  5.80 | ppl   330.34\n",
      "| epoch   1 |   680/ 3195 batches | lr 5.00 | ms/batch 239.15 | loss  5.86 | ppl   349.57\n",
      "| epoch   1 |   690/ 3195 batches | lr 5.00 | ms/batch 239.31 | loss  5.70 | ppl   297.57\n",
      "| epoch   1 |   700/ 3195 batches | lr 5.00 | ms/batch 236.25 | loss  5.95 | ppl   382.52\n",
      "| epoch   1 |   710/ 3195 batches | lr 5.00 | ms/batch 236.49 | loss  5.95 | ppl   383.17\n",
      "| epoch   1 |   720/ 3195 batches | lr 5.00 | ms/batch 235.82 | loss  5.81 | ppl   332.62\n",
      "| epoch   1 |   730/ 3195 batches | lr 5.00 | ms/batch 233.45 | loss  5.82 | ppl   337.49\n",
      "| epoch   1 |   740/ 3195 batches | lr 5.00 | ms/batch 239.84 | loss  5.78 | ppl   324.94\n",
      "| epoch   1 |   750/ 3195 batches | lr 5.00 | ms/batch 236.04 | loss  5.80 | ppl   331.74\n",
      "| epoch   1 |   760/ 3195 batches | lr 5.00 | ms/batch 238.85 | loss  5.66 | ppl   286.01\n",
      "| epoch   1 |   770/ 3195 batches | lr 5.00 | ms/batch 234.99 | loss  5.68 | ppl   291.60\n",
      "| epoch   1 |   780/ 3195 batches | lr 5.00 | ms/batch 229.58 | loss  5.72 | ppl   305.08\n",
      "| epoch   1 |   790/ 3195 batches | lr 5.00 | ms/batch 240.93 | loss  5.76 | ppl   318.92\n",
      "| epoch   1 |   800/ 3195 batches | lr 5.00 | ms/batch 237.89 | loss  5.60 | ppl   269.26\n",
      "| epoch   1 |   810/ 3195 batches | lr 5.00 | ms/batch 242.60 | loss  5.94 | ppl   379.67\n",
      "| epoch   1 |   820/ 3195 batches | lr 5.00 | ms/batch 237.47 | loss  5.76 | ppl   317.71\n",
      "| epoch   1 |   830/ 3195 batches | lr 5.00 | ms/batch 239.25 | loss  5.66 | ppl   286.21\n",
      "| epoch   1 |   840/ 3195 batches | lr 5.00 | ms/batch 239.18 | loss  5.94 | ppl   379.14\n",
      "| epoch   1 |   850/ 3195 batches | lr 5.00 | ms/batch 233.55 | loss  5.88 | ppl   358.30\n",
      "| epoch   1 |   860/ 3195 batches | lr 5.00 | ms/batch 238.93 | loss  5.83 | ppl   338.82\n",
      "| epoch   1 |   870/ 3195 batches | lr 5.00 | ms/batch 248.27 | loss  5.75 | ppl   314.83\n",
      "| epoch   1 |   880/ 3195 batches | lr 5.00 | ms/batch 250.14 | loss  5.75 | ppl   313.71\n",
      "| epoch   1 |   890/ 3195 batches | lr 5.00 | ms/batch 236.26 | loss  5.86 | ppl   352.26\n",
      "| epoch   1 |   900/ 3195 batches | lr 5.00 | ms/batch 236.52 | loss  5.87 | ppl   355.53\n",
      "| epoch   1 |   910/ 3195 batches | lr 5.00 | ms/batch 235.23 | loss  5.61 | ppl   273.50\n",
      "| epoch   1 |   920/ 3195 batches | lr 5.00 | ms/batch 240.54 | loss  5.83 | ppl   338.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   930/ 3195 batches | lr 5.00 | ms/batch 237.59 | loss  5.81 | ppl   333.86\n",
      "| epoch   1 |   940/ 3195 batches | lr 5.00 | ms/batch 239.00 | loss  5.90 | ppl   364.86\n",
      "| epoch   1 |   950/ 3195 batches | lr 5.00 | ms/batch 235.95 | loss  5.88 | ppl   357.35\n",
      "| epoch   1 |   960/ 3195 batches | lr 5.00 | ms/batch 235.05 | loss  5.73 | ppl   308.47\n",
      "| epoch   1 |   970/ 3195 batches | lr 5.00 | ms/batch 242.23 | loss  5.84 | ppl   344.01\n",
      "| epoch   1 |   980/ 3195 batches | lr 5.00 | ms/batch 234.07 | loss  5.67 | ppl   290.34\n",
      "| epoch   1 |   990/ 3195 batches | lr 5.00 | ms/batch 236.97 | loss  5.92 | ppl   373.88\n",
      "| epoch   1 |  1000/ 3195 batches | lr 5.00 | ms/batch 237.29 | loss  5.74 | ppl   312.37\n",
      "| epoch   1 |  1010/ 3195 batches | lr 5.00 | ms/batch 239.82 | loss  5.93 | ppl   374.87\n",
      "| epoch   1 |  1020/ 3195 batches | lr 5.00 | ms/batch 244.60 | loss  5.84 | ppl   342.71\n",
      "| epoch   1 |  1030/ 3195 batches | lr 5.00 | ms/batch 250.10 | loss  5.91 | ppl   368.29\n",
      "| epoch   1 |  1040/ 3195 batches | lr 5.00 | ms/batch 242.44 | loss  5.77 | ppl   321.05\n",
      "| epoch   1 |  1050/ 3195 batches | lr 5.00 | ms/batch 239.22 | loss  5.74 | ppl   311.83\n",
      "| epoch   1 |  1060/ 3195 batches | lr 5.00 | ms/batch 245.09 | loss  5.88 | ppl   358.35\n",
      "| epoch   1 |  1070/ 3195 batches | lr 5.00 | ms/batch 244.10 | loss  5.80 | ppl   330.61\n",
      "| epoch   1 |  1080/ 3195 batches | lr 5.00 | ms/batch 241.67 | loss  5.81 | ppl   333.44\n",
      "| epoch   1 |  1090/ 3195 batches | lr 5.00 | ms/batch 237.99 | loss  5.88 | ppl   356.53\n",
      "| epoch   1 |  1100/ 3195 batches | lr 5.00 | ms/batch 242.53 | loss  5.80 | ppl   331.63\n",
      "| epoch   1 |  1110/ 3195 batches | lr 5.00 | ms/batch 237.01 | loss  5.69 | ppl   296.52\n",
      "| epoch   1 |  1120/ 3195 batches | lr 5.00 | ms/batch 235.48 | loss  5.74 | ppl   310.15\n",
      "| epoch   1 |  1130/ 3195 batches | lr 5.00 | ms/batch 244.52 | loss  5.53 | ppl   253.04\n",
      "| epoch   1 |  1140/ 3195 batches | lr 5.00 | ms/batch 242.99 | loss  5.70 | ppl   297.52\n",
      "| epoch   1 |  1150/ 3195 batches | lr 5.00 | ms/batch 244.76 | loss  5.72 | ppl   304.60\n",
      "| epoch   1 |  1160/ 3195 batches | lr 5.00 | ms/batch 238.18 | loss  5.70 | ppl   299.78\n",
      "| epoch   1 |  1170/ 3195 batches | lr 5.00 | ms/batch 241.29 | loss  5.62 | ppl   276.20\n",
      "| epoch   1 |  1180/ 3195 batches | lr 5.00 | ms/batch 245.93 | loss  5.74 | ppl   309.61\n",
      "| epoch   1 |  1190/ 3195 batches | lr 5.00 | ms/batch 237.42 | loss  5.65 | ppl   283.49\n",
      "| epoch   1 |  1200/ 3195 batches | lr 5.00 | ms/batch 239.38 | loss  5.70 | ppl   299.43\n",
      "| epoch   1 |  1210/ 3195 batches | lr 5.00 | ms/batch 235.18 | loss  5.77 | ppl   320.75\n",
      "| epoch   1 |  1220/ 3195 batches | lr 5.00 | ms/batch 241.30 | loss  5.73 | ppl   307.23\n",
      "| epoch   1 |  1230/ 3195 batches | lr 5.00 | ms/batch 239.32 | loss  5.71 | ppl   303.14\n",
      "| epoch   1 |  1240/ 3195 batches | lr 5.00 | ms/batch 243.03 | loss  5.82 | ppl   336.47\n",
      "| epoch   1 |  1250/ 3195 batches | lr 5.00 | ms/batch 241.93 | loss  5.79 | ppl   328.15\n",
      "| epoch   1 |  1260/ 3195 batches | lr 5.00 | ms/batch 236.51 | loss  5.74 | ppl   311.53\n",
      "| epoch   1 |  1270/ 3195 batches | lr 5.00 | ms/batch 244.39 | loss  5.67 | ppl   288.92\n",
      "| epoch   1 |  1280/ 3195 batches | lr 5.00 | ms/batch 241.15 | loss  5.68 | ppl   292.30\n",
      "| epoch   1 |  1290/ 3195 batches | lr 5.00 | ms/batch 239.58 | loss  5.61 | ppl   273.78\n",
      "| epoch   1 |  1300/ 3195 batches | lr 5.00 | ms/batch 229.91 | loss  5.62 | ppl   274.58\n",
      "| epoch   1 |  1310/ 3195 batches | lr 5.00 | ms/batch 232.53 | loss  5.85 | ppl   347.56\n",
      "| epoch   1 |  1320/ 3195 batches | lr 5.00 | ms/batch 237.31 | loss  5.74 | ppl   312.52\n",
      "| epoch   1 |  1330/ 3195 batches | lr 5.00 | ms/batch 234.93 | loss  5.83 | ppl   340.64\n",
      "| epoch   1 |  1340/ 3195 batches | lr 5.00 | ms/batch 243.52 | loss  5.69 | ppl   294.47\n",
      "| epoch   1 |  1350/ 3195 batches | lr 5.00 | ms/batch 240.94 | loss  5.69 | ppl   294.62\n",
      "| epoch   1 |  1360/ 3195 batches | lr 5.00 | ms/batch 238.05 | loss  5.61 | ppl   272.23\n",
      "| epoch   1 |  1370/ 3195 batches | lr 5.00 | ms/batch 233.56 | loss  5.84 | ppl   342.37\n",
      "| epoch   1 |  1380/ 3195 batches | lr 5.00 | ms/batch 240.32 | loss  5.51 | ppl   245.95\n",
      "| epoch   1 |  1390/ 3195 batches | lr 5.00 | ms/batch 238.11 | loss  5.74 | ppl   310.44\n",
      "| epoch   1 |  1400/ 3195 batches | lr 5.00 | ms/batch 238.13 | loss  5.57 | ppl   263.31\n",
      "| epoch   1 |  1410/ 3195 batches | lr 5.00 | ms/batch 250.72 | loss  5.57 | ppl   263.17\n",
      "| epoch   1 |  1420/ 3195 batches | lr 5.00 | ms/batch 242.76 | loss  5.63 | ppl   277.75\n",
      "| epoch   1 |  1430/ 3195 batches | lr 5.00 | ms/batch 241.66 | loss  5.48 | ppl   240.62\n",
      "| epoch   1 |  1440/ 3195 batches | lr 5.00 | ms/batch 238.31 | loss  5.54 | ppl   255.17\n",
      "| epoch   1 |  1450/ 3195 batches | lr 5.00 | ms/batch 238.62 | loss  5.49 | ppl   243.25\n",
      "| epoch   1 |  1460/ 3195 batches | lr 5.00 | ms/batch 254.74 | loss  5.49 | ppl   242.97\n",
      "| epoch   1 |  1470/ 3195 batches | lr 5.00 | ms/batch 244.51 | loss  5.58 | ppl   265.54\n",
      "| epoch   1 |  1480/ 3195 batches | lr 5.00 | ms/batch 244.47 | loss  5.40 | ppl   221.87\n",
      "| epoch   1 |  1490/ 3195 batches | lr 5.00 | ms/batch 236.83 | loss  5.65 | ppl   284.74\n",
      "| epoch   1 |  1500/ 3195 batches | lr 5.00 | ms/batch 237.75 | loss  5.80 | ppl   331.91\n",
      "| epoch   1 |  1510/ 3195 batches | lr 5.00 | ms/batch 236.14 | loss  5.67 | ppl   289.23\n",
      "| epoch   1 |  1520/ 3195 batches | lr 5.00 | ms/batch 239.02 | loss  5.66 | ppl   287.60\n",
      "| epoch   1 |  1530/ 3195 batches | lr 5.00 | ms/batch 234.62 | loss  5.57 | ppl   261.71\n",
      "| epoch   1 |  1540/ 3195 batches | lr 5.00 | ms/batch 239.18 | loss  5.64 | ppl   281.74\n",
      "| epoch   1 |  1550/ 3195 batches | lr 5.00 | ms/batch 242.32 | loss  5.68 | ppl   293.60\n",
      "| epoch   1 |  1560/ 3195 batches | lr 5.00 | ms/batch 243.44 | loss  5.55 | ppl   257.61\n",
      "| epoch   1 |  1570/ 3195 batches | lr 5.00 | ms/batch 250.85 | loss  5.55 | ppl   258.31\n",
      "| epoch   1 |  1580/ 3195 batches | lr 5.00 | ms/batch 240.63 | loss  5.71 | ppl   300.79\n",
      "| epoch   1 |  1590/ 3195 batches | lr 5.00 | ms/batch 239.95 | loss  5.77 | ppl   319.15\n",
      "| epoch   1 |  1600/ 3195 batches | lr 5.00 | ms/batch 251.59 | loss  5.57 | ppl   261.95\n",
      "| epoch   1 |  1610/ 3195 batches | lr 5.00 | ms/batch 250.29 | loss  5.73 | ppl   308.14\n",
      "| epoch   1 |  1620/ 3195 batches | lr 5.00 | ms/batch 248.80 | loss  5.71 | ppl   302.76\n",
      "| epoch   1 |  1630/ 3195 batches | lr 5.00 | ms/batch 237.42 | loss  5.72 | ppl   304.21\n",
      "| epoch   1 |  1640/ 3195 batches | lr 5.00 | ms/batch 249.12 | loss  5.77 | ppl   320.91\n",
      "| epoch   1 |  1650/ 3195 batches | lr 5.00 | ms/batch 245.07 | loss  5.68 | ppl   292.33\n",
      "| epoch   1 |  1660/ 3195 batches | lr 5.00 | ms/batch 241.12 | loss  5.65 | ppl   284.84\n",
      "| epoch   1 |  1670/ 3195 batches | lr 5.00 | ms/batch 243.82 | loss  5.56 | ppl   258.99\n",
      "| epoch   1 |  1680/ 3195 batches | lr 5.00 | ms/batch 236.29 | loss  5.56 | ppl   259.12\n",
      "| epoch   1 |  1690/ 3195 batches | lr 5.00 | ms/batch 237.19 | loss  5.63 | ppl   279.60\n",
      "| epoch   1 |  1700/ 3195 batches | lr 5.00 | ms/batch 236.23 | loss  5.61 | ppl   273.93\n",
      "| epoch   1 |  1710/ 3195 batches | lr 5.00 | ms/batch 238.41 | loss  5.69 | ppl   296.81\n",
      "| epoch   1 |  1720/ 3195 batches | lr 5.00 | ms/batch 253.14 | loss  5.51 | ppl   247.42\n",
      "| epoch   1 |  1730/ 3195 batches | lr 5.00 | ms/batch 261.34 | loss  5.61 | ppl   272.60\n",
      "| epoch   1 |  1740/ 3195 batches | lr 5.00 | ms/batch 248.76 | loss  5.66 | ppl   287.76\n",
      "| epoch   1 |  1750/ 3195 batches | lr 5.00 | ms/batch 267.44 | loss  5.70 | ppl   297.83\n",
      "| epoch   1 |  1760/ 3195 batches | lr 5.00 | ms/batch 283.68 | loss  5.54 | ppl   253.96\n",
      "| epoch   1 |  1770/ 3195 batches | lr 5.00 | ms/batch 258.73 | loss  5.62 | ppl   277.05\n",
      "| epoch   1 |  1780/ 3195 batches | lr 5.00 | ms/batch 266.38 | loss  5.55 | ppl   257.10\n",
      "| epoch   1 |  1790/ 3195 batches | lr 5.00 | ms/batch 261.49 | loss  5.53 | ppl   252.38\n",
      "| epoch   1 |  1800/ 3195 batches | lr 5.00 | ms/batch 262.30 | loss  5.58 | ppl   264.31\n",
      "| epoch   1 |  1810/ 3195 batches | lr 5.00 | ms/batch 257.20 | loss  5.61 | ppl   273.57\n",
      "| epoch   1 |  1820/ 3195 batches | lr 5.00 | ms/batch 250.56 | loss  5.62 | ppl   276.40\n",
      "| epoch   1 |  1830/ 3195 batches | lr 5.00 | ms/batch 249.53 | loss  5.65 | ppl   285.61\n",
      "| epoch   1 |  1840/ 3195 batches | lr 5.00 | ms/batch 247.70 | loss  5.61 | ppl   273.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  1850/ 3195 batches | lr 5.00 | ms/batch 257.70 | loss  5.64 | ppl   282.84\n",
      "| epoch   1 |  1860/ 3195 batches | lr 5.00 | ms/batch 266.48 | loss  5.68 | ppl   293.27\n",
      "| epoch   1 |  1870/ 3195 batches | lr 5.00 | ms/batch 252.62 | loss  5.64 | ppl   281.19\n",
      "| epoch   1 |  1880/ 3195 batches | lr 5.00 | ms/batch 246.18 | loss  5.79 | ppl   328.48\n",
      "| epoch   1 |  1890/ 3195 batches | lr 5.00 | ms/batch 250.06 | loss  5.68 | ppl   292.86\n",
      "| epoch   1 |  1900/ 3195 batches | lr 5.00 | ms/batch 245.29 | loss  5.67 | ppl   289.37\n",
      "| epoch   1 |  1910/ 3195 batches | lr 5.00 | ms/batch 236.34 | loss  5.64 | ppl   282.76\n",
      "| epoch   1 |  1920/ 3195 batches | lr 5.00 | ms/batch 240.82 | loss  5.53 | ppl   251.50\n",
      "| epoch   1 |  1930/ 3195 batches | lr 5.00 | ms/batch 240.14 | loss  5.66 | ppl   288.35\n",
      "| epoch   1 |  1940/ 3195 batches | lr 5.00 | ms/batch 240.16 | loss  5.62 | ppl   275.84\n",
      "| epoch   1 |  1950/ 3195 batches | lr 5.00 | ms/batch 239.12 | loss  5.53 | ppl   250.99\n",
      "| epoch   1 |  1960/ 3195 batches | lr 5.00 | ms/batch 236.58 | loss  5.52 | ppl   248.45\n",
      "| epoch   1 |  1970/ 3195 batches | lr 5.00 | ms/batch 243.37 | loss  5.62 | ppl   276.31\n",
      "| epoch   1 |  1980/ 3195 batches | lr 5.00 | ms/batch 233.38 | loss  5.64 | ppl   280.90\n",
      "| epoch   1 |  1990/ 3195 batches | lr 5.00 | ms/batch 242.39 | loss  5.68 | ppl   292.78\n",
      "| epoch   1 |  2000/ 3195 batches | lr 5.00 | ms/batch 235.41 | loss  5.76 | ppl   318.60\n",
      "| epoch   1 |  2010/ 3195 batches | lr 5.00 | ms/batch 242.14 | loss  5.64 | ppl   282.21\n",
      "| epoch   1 |  2020/ 3195 batches | lr 5.00 | ms/batch 246.84 | loss  5.65 | ppl   284.80\n",
      "| epoch   1 |  2030/ 3195 batches | lr 5.00 | ms/batch 237.83 | loss  5.60 | ppl   269.37\n",
      "| epoch   1 |  2040/ 3195 batches | lr 5.00 | ms/batch 239.69 | loss  5.61 | ppl   272.55\n",
      "| epoch   1 |  2050/ 3195 batches | lr 5.00 | ms/batch 237.93 | loss  5.57 | ppl   262.79\n",
      "| epoch   1 |  2060/ 3195 batches | lr 5.00 | ms/batch 238.14 | loss  5.48 | ppl   238.88\n",
      "| epoch   1 |  2070/ 3195 batches | lr 5.00 | ms/batch 243.86 | loss  5.59 | ppl   268.34\n",
      "| epoch   1 |  2080/ 3195 batches | lr 5.00 | ms/batch 239.22 | loss  5.45 | ppl   232.40\n",
      "| epoch   1 |  2090/ 3195 batches | lr 5.00 | ms/batch 242.32 | loss  5.45 | ppl   232.60\n",
      "| epoch   1 |  2100/ 3195 batches | lr 5.00 | ms/batch 238.19 | loss  5.57 | ppl   261.55\n",
      "| epoch   1 |  2110/ 3195 batches | lr 5.00 | ms/batch 251.99 | loss  5.53 | ppl   252.83\n",
      "| epoch   1 |  2120/ 3195 batches | lr 5.00 | ms/batch 249.48 | loss  5.51 | ppl   247.09\n",
      "| epoch   1 |  2130/ 3195 batches | lr 5.00 | ms/batch 256.16 | loss  5.61 | ppl   272.29\n",
      "| epoch   1 |  2140/ 3195 batches | lr 5.00 | ms/batch 263.35 | loss  5.51 | ppl   246.78\n",
      "| epoch   1 |  2150/ 3195 batches | lr 5.00 | ms/batch 254.38 | loss  5.62 | ppl   276.91\n",
      "| epoch   1 |  2160/ 3195 batches | lr 5.00 | ms/batch 259.08 | loss  5.68 | ppl   292.60\n",
      "| epoch   1 |  2170/ 3195 batches | lr 5.00 | ms/batch 273.45 | loss  5.59 | ppl   268.32\n",
      "| epoch   1 |  2180/ 3195 batches | lr 5.00 | ms/batch 260.49 | loss  5.65 | ppl   284.27\n",
      "| epoch   1 |  2190/ 3195 batches | lr 5.00 | ms/batch 272.61 | loss  5.56 | ppl   260.96\n",
      "| epoch   1 |  2200/ 3195 batches | lr 5.00 | ms/batch 265.20 | loss  5.59 | ppl   266.96\n",
      "| epoch   1 |  2210/ 3195 batches | lr 5.00 | ms/batch 256.05 | loss  5.61 | ppl   271.94\n",
      "| epoch   1 |  2220/ 3195 batches | lr 5.00 | ms/batch 248.56 | loss  5.56 | ppl   259.65\n",
      "| epoch   1 |  2230/ 3195 batches | lr 5.00 | ms/batch 242.05 | loss  5.56 | ppl   259.77\n",
      "| epoch   1 |  2240/ 3195 batches | lr 5.00 | ms/batch 238.50 | loss  5.45 | ppl   232.03\n",
      "| epoch   1 |  2250/ 3195 batches | lr 5.00 | ms/batch 243.56 | loss  5.51 | ppl   247.28\n",
      "| epoch   1 |  2260/ 3195 batches | lr 5.00 | ms/batch 239.75 | loss  5.52 | ppl   248.86\n",
      "| epoch   1 |  2270/ 3195 batches | lr 5.00 | ms/batch 238.73 | loss  5.48 | ppl   239.39\n",
      "| epoch   1 |  2280/ 3195 batches | lr 5.00 | ms/batch 252.51 | loss  5.41 | ppl   223.69\n",
      "| epoch   1 |  2290/ 3195 batches | lr 5.00 | ms/batch 246.62 | loss  5.40 | ppl   220.96\n",
      "| epoch   1 |  2300/ 3195 batches | lr 5.00 | ms/batch 251.36 | loss  5.42 | ppl   225.43\n",
      "| epoch   1 |  2310/ 3195 batches | lr 5.00 | ms/batch 250.25 | loss  5.48 | ppl   239.72\n",
      "| epoch   1 |  2320/ 3195 batches | lr 5.00 | ms/batch 254.01 | loss  5.52 | ppl   248.69\n",
      "| epoch   1 |  2330/ 3195 batches | lr 5.00 | ms/batch 254.97 | loss  5.39 | ppl   219.21\n",
      "| epoch   1 |  2340/ 3195 batches | lr 5.00 | ms/batch 251.33 | loss  5.44 | ppl   229.36\n",
      "| epoch   1 |  2350/ 3195 batches | lr 5.00 | ms/batch 259.24 | loss  5.50 | ppl   244.11\n",
      "| epoch   1 |  2360/ 3195 batches | lr 5.00 | ms/batch 264.90 | loss  5.26 | ppl   192.72\n",
      "| epoch   1 |  2370/ 3195 batches | lr 5.00 | ms/batch 259.70 | loss  5.45 | ppl   232.44\n",
      "| epoch   1 |  2380/ 3195 batches | lr 5.00 | ms/batch 241.22 | loss  5.41 | ppl   222.65\n",
      "| epoch   1 |  2390/ 3195 batches | lr 5.00 | ms/batch 271.68 | loss  5.46 | ppl   234.92\n",
      "| epoch   1 |  2400/ 3195 batches | lr 5.00 | ms/batch 271.35 | loss  5.50 | ppl   244.93\n",
      "| epoch   1 |  2410/ 3195 batches | lr 5.00 | ms/batch 253.57 | loss  5.55 | ppl   256.34\n",
      "| epoch   1 |  2420/ 3195 batches | lr 5.00 | ms/batch 245.57 | loss  5.49 | ppl   242.39\n",
      "| epoch   1 |  2430/ 3195 batches | lr 5.00 | ms/batch 241.90 | loss  5.64 | ppl   280.52\n",
      "| epoch   1 |  2440/ 3195 batches | lr 5.00 | ms/batch 246.57 | loss  5.49 | ppl   241.29\n",
      "| epoch   1 |  2450/ 3195 batches | lr 5.00 | ms/batch 247.60 | loss  5.46 | ppl   236.11\n",
      "| epoch   1 |  2460/ 3195 batches | lr 5.00 | ms/batch 241.45 | loss  5.60 | ppl   270.35\n",
      "| epoch   1 |  2470/ 3195 batches | lr 5.00 | ms/batch 243.64 | loss  5.54 | ppl   254.50\n",
      "| epoch   1 |  2480/ 3195 batches | lr 5.00 | ms/batch 247.80 | loss  5.65 | ppl   283.11\n",
      "| epoch   1 |  2490/ 3195 batches | lr 5.00 | ms/batch 259.76 | loss  5.67 | ppl   290.07\n",
      "| epoch   1 |  2500/ 3195 batches | lr 5.00 | ms/batch 261.72 | loss  5.52 | ppl   249.73\n",
      "| epoch   1 |  2510/ 3195 batches | lr 5.00 | ms/batch 264.37 | loss  5.41 | ppl   223.43\n",
      "| epoch   1 |  2520/ 3195 batches | lr 5.00 | ms/batch 264.68 | loss  5.51 | ppl   247.43\n",
      "| epoch   1 |  2530/ 3195 batches | lr 5.00 | ms/batch 258.95 | loss  5.52 | ppl   248.88\n",
      "| epoch   1 |  2540/ 3195 batches | lr 5.00 | ms/batch 246.65 | loss  5.48 | ppl   239.59\n",
      "| epoch   1 |  2550/ 3195 batches | lr 5.00 | ms/batch 246.86 | loss  5.54 | ppl   255.16\n",
      "| epoch   1 |  2560/ 3195 batches | lr 5.00 | ms/batch 239.62 | loss  5.57 | ppl   261.26\n",
      "| epoch   1 |  2570/ 3195 batches | lr 5.00 | ms/batch 248.74 | loss  5.59 | ppl   266.67\n",
      "| epoch   1 |  2580/ 3195 batches | lr 5.00 | ms/batch 248.94 | loss  5.55 | ppl   257.95\n",
      "| epoch   1 |  2590/ 3195 batches | lr 5.00 | ms/batch 241.48 | loss  5.38 | ppl   217.79\n",
      "| epoch   1 |  2600/ 3195 batches | lr 5.00 | ms/batch 243.79 | loss  5.58 | ppl   266.37\n",
      "| epoch   1 |  2610/ 3195 batches | lr 5.00 | ms/batch 242.86 | loss  5.52 | ppl   249.73\n",
      "| epoch   1 |  2620/ 3195 batches | lr 5.00 | ms/batch 239.62 | loss  5.56 | ppl   260.01\n",
      "| epoch   1 |  2630/ 3195 batches | lr 5.00 | ms/batch 241.68 | loss  5.56 | ppl   259.69\n",
      "| epoch   1 |  2640/ 3195 batches | lr 5.00 | ms/batch 243.23 | loss  5.52 | ppl   249.53\n",
      "| epoch   1 |  2650/ 3195 batches | lr 5.00 | ms/batch 243.98 | loss  5.46 | ppl   236.04\n",
      "| epoch   1 |  2660/ 3195 batches | lr 5.00 | ms/batch 243.70 | loss  5.46 | ppl   236.04\n",
      "| epoch   1 |  2670/ 3195 batches | lr 5.00 | ms/batch 243.05 | loss  5.59 | ppl   266.60\n",
      "| epoch   1 |  2680/ 3195 batches | lr 5.00 | ms/batch 244.42 | loss  5.49 | ppl   241.13\n",
      "| epoch   1 |  2690/ 3195 batches | lr 5.00 | ms/batch 244.35 | loss  5.53 | ppl   251.23\n",
      "| epoch   1 |  2700/ 3195 batches | lr 5.00 | ms/batch 249.59 | loss  5.55 | ppl   256.31\n",
      "| epoch   1 |  2710/ 3195 batches | lr 5.00 | ms/batch 245.02 | loss  5.58 | ppl   265.54\n",
      "| epoch   1 |  2720/ 3195 batches | lr 5.00 | ms/batch 241.48 | loss  5.57 | ppl   263.08\n",
      "| epoch   1 |  2730/ 3195 batches | lr 5.00 | ms/batch 253.52 | loss  5.36 | ppl   212.25\n",
      "| epoch   1 |  2740/ 3195 batches | lr 5.00 | ms/batch 251.33 | loss  5.57 | ppl   262.15\n",
      "| epoch   1 |  2750/ 3195 batches | lr 5.00 | ms/batch 246.29 | loss  5.58 | ppl   266.05\n",
      "| epoch   1 |  2760/ 3195 batches | lr 5.00 | ms/batch 249.71 | loss  5.50 | ppl   245.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  2770/ 3195 batches | lr 5.00 | ms/batch 259.28 | loss  5.51 | ppl   246.75\n",
      "| epoch   1 |  2780/ 3195 batches | lr 5.00 | ms/batch 238.92 | loss  5.48 | ppl   240.38\n",
      "| epoch   1 |  2790/ 3195 batches | lr 5.00 | ms/batch 251.81 | loss  5.42 | ppl   225.04\n",
      "| epoch   1 |  2800/ 3195 batches | lr 5.00 | ms/batch 250.36 | loss  5.58 | ppl   265.57\n",
      "| epoch   1 |  2810/ 3195 batches | lr 5.00 | ms/batch 244.60 | loss  5.44 | ppl   231.44\n",
      "| epoch   1 |  2820/ 3195 batches | lr 5.00 | ms/batch 240.03 | loss  5.40 | ppl   221.86\n",
      "| epoch   1 |  2830/ 3195 batches | lr 5.00 | ms/batch 242.30 | loss  5.36 | ppl   211.85\n",
      "| epoch   1 |  2840/ 3195 batches | lr 5.00 | ms/batch 240.94 | loss  5.34 | ppl   208.94\n",
      "| epoch   1 |  2850/ 3195 batches | lr 5.00 | ms/batch 246.42 | loss  5.28 | ppl   195.70\n",
      "| epoch   1 |  2860/ 3195 batches | lr 5.00 | ms/batch 239.85 | loss  5.37 | ppl   215.92\n",
      "| epoch   1 |  2870/ 3195 batches | lr 5.00 | ms/batch 243.56 | loss  5.39 | ppl   219.16\n",
      "| epoch   1 |  2880/ 3195 batches | lr 5.00 | ms/batch 240.77 | loss  5.44 | ppl   229.46\n",
      "| epoch   1 |  2890/ 3195 batches | lr 5.00 | ms/batch 239.67 | loss  5.41 | ppl   223.90\n",
      "| epoch   1 |  2900/ 3195 batches | lr 5.00 | ms/batch 246.97 | loss  5.37 | ppl   214.92\n",
      "| epoch   1 |  2910/ 3195 batches | lr 5.00 | ms/batch 249.93 | loss  5.43 | ppl   228.03\n",
      "| epoch   1 |  2920/ 3195 batches | lr 5.00 | ms/batch 247.14 | loss  5.46 | ppl   235.70\n",
      "| epoch   1 |  2930/ 3195 batches | lr 5.00 | ms/batch 249.16 | loss  5.49 | ppl   241.34\n",
      "| epoch   1 |  2940/ 3195 batches | lr 5.00 | ms/batch 244.74 | loss  5.55 | ppl   256.90\n",
      "| epoch   1 |  2950/ 3195 batches | lr 5.00 | ms/batch 246.88 | loss  5.45 | ppl   233.10\n",
      "| epoch   1 |  2960/ 3195 batches | lr 5.00 | ms/batch 249.01 | loss  5.43 | ppl   228.31\n",
      "| epoch   1 |  2970/ 3195 batches | lr 5.00 | ms/batch 244.24 | loss  5.31 | ppl   203.15\n",
      "| epoch   1 |  2980/ 3195 batches | lr 5.00 | ms/batch 249.20 | loss  5.49 | ppl   243.02\n",
      "| epoch   1 |  2990/ 3195 batches | lr 5.00 | ms/batch 240.86 | loss  5.51 | ppl   248.03\n",
      "| epoch   1 |  3000/ 3195 batches | lr 5.00 | ms/batch 239.83 | loss  5.40 | ppl   221.72\n",
      "| epoch   1 |  3010/ 3195 batches | lr 5.00 | ms/batch 240.87 | loss  5.47 | ppl   236.80\n",
      "| epoch   1 |  3020/ 3195 batches | lr 5.00 | ms/batch 254.22 | loss  5.44 | ppl   231.41\n",
      "| epoch   1 |  3030/ 3195 batches | lr 5.00 | ms/batch 247.66 | loss  5.36 | ppl   213.02\n",
      "| epoch   1 |  3040/ 3195 batches | lr 5.00 | ms/batch 245.46 | loss  5.51 | ppl   247.31\n",
      "| epoch   1 |  3050/ 3195 batches | lr 5.00 | ms/batch 245.75 | loss  5.49 | ppl   241.83\n",
      "| epoch   1 |  3060/ 3195 batches | lr 5.00 | ms/batch 240.14 | loss  5.42 | ppl   225.66\n",
      "| epoch   1 |  3070/ 3195 batches | lr 5.00 | ms/batch 240.51 | loss  5.54 | ppl   255.39\n",
      "| epoch   1 |  3080/ 3195 batches | lr 5.00 | ms/batch 261.74 | loss  5.42 | ppl   224.85\n",
      "| epoch   1 |  3090/ 3195 batches | lr 5.00 | ms/batch 263.69 | loss  5.32 | ppl   203.53\n",
      "| epoch   1 |  3100/ 3195 batches | lr 5.00 | ms/batch 269.73 | loss  5.66 | ppl   285.94\n",
      "| epoch   1 |  3110/ 3195 batches | lr 5.00 | ms/batch 250.68 | loss  5.28 | ppl   195.76\n",
      "| epoch   1 |  3120/ 3195 batches | lr 5.00 | ms/batch 262.88 | loss  5.43 | ppl   228.91\n",
      "| epoch   1 |  3130/ 3195 batches | lr 5.00 | ms/batch 261.16 | loss  5.38 | ppl   216.18\n",
      "| epoch   1 |  3140/ 3195 batches | lr 5.00 | ms/batch 256.33 | loss  5.44 | ppl   229.93\n",
      "| epoch   1 |  3150/ 3195 batches | lr 5.00 | ms/batch 243.66 | loss  5.48 | ppl   238.83\n",
      "| epoch   1 |  3160/ 3195 batches | lr 5.00 | ms/batch 248.13 | loss  5.32 | ppl   204.68\n",
      "| epoch   1 |  3170/ 3195 batches | lr 5.00 | ms/batch 242.92 | loss  5.45 | ppl   233.26\n",
      "| epoch   1 |  3180/ 3195 batches | lr 5.00 | ms/batch 246.32 | loss  5.31 | ppl   201.93\n",
      "| epoch   1 |  3190/ 3195 batches | lr 5.00 | ms/batch 254.45 | loss  5.34 | ppl   208.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 856.42s | valid loss  5.12 | valid ppl   168.12\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 1 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(\"starting\")\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgNfblVjucsp"
   },
   "source": [
    "#### 2.3.2 Why do we need to use `torch.nn.utils.clip_grad_norm_` in training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`[double click here to add a solution]`**\n",
    "\n",
    "To clip gradient avoid Gradient explosion"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c3127b33eef405e17321b7de0d4f8a216c3d52d380e39a853504fe3a37af8cd4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
